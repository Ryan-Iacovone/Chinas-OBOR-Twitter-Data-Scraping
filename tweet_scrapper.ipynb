{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to scrap tweets from twitter using search terms defined by the twitter search parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "# define the search term (I'm very happy with these search terms so far)\n",
    "search_term = \"(#OBOR OR #BeltandRoad OR #BeltandRoadInitiative OR #OneBeltOneRoad)\"\n",
    "#Define the timespan to look for tweets \n",
    "since_date = \"2021-01-01\" #from\n",
    "until_date = \"2021-12-31\"   #to\n",
    "\n",
    "#7.5 min for 10k tweets + 27 min for 45k tweets\n",
    "limit = 100000\n",
    "\n",
    "# create an empty list to store the tweets\n",
    "tweets_all_list = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "# use snscrape to scrape tweets\n",
    "for tweet in sntwitter.TwitterSearchScraper(f\"{search_term} since:{since_date} until:{until_date}\").get_items():\n",
    "\n",
    "    # append the tweet information to the list                                                      #Languages are categorized by \"ISO 639-1 codes\"\n",
    "    tweets_all_list.append([tweet.id, tweet.date, tweet.content, tweet.user.username, tweet.url, tweet.hashtags, tweet.lang])\n",
    "\n",
    "    count += 1  # increment the counter variable\n",
    "\n",
    "    if count == limit:  # break out of the loop once 100000 tweets have been scraped\n",
    "        print(\"\\nTweets have finsihed loading\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Loading Tweet Number: {count}\")\n",
    "\n",
    "\n",
    "# convert the list of lists into a pandas DataFrame\n",
    "tweets_all = pd.DataFrame(tweets_all_list, columns=['id', 'date', 'content', 'username', 'url', 'hashtag', 'language'])\n",
    "\n",
    "#just cleaning up the variable view \n",
    "del tweets_all_list, tweet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some basic investigation into the tweets dataframe to make sure it looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping each tweet by lanuage to see how many lanauges are represented to see how many we should focus on \n",
    "re = tweets_all.groupby('language').size().reset_index(name='count').sort_values('count', ascending=False)\n",
    "re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a data frame tweets based on a particular language \n",
    "#tweets_filter= tweets_all[(tweets_all[\"language\"] == \"en\")] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the number of tweets posted by each person \n",
    "unqiue_tweeters = tweets_all['username'].value_counts()\n",
    "unqiue_tweeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin = tweets_all[(tweets_all['username'] != \"youloveBri_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking all the tweets of our number 1 tweeter to see if they're all unique\n",
    "only_beta = tweets_all[(tweets_all['username'] == \"youloveBri_\")]\n",
    "seeing = only_beta['content'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing some quick preprocessing of the dataframe before making it into an excel file and loading it into R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the timezone from that date format automatically gathered by SNScrape (only run once)\n",
    "tweets_all['date'] = tweets_all['date'].dt.tz_convert(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the date to a new format changes it to a string instead of a date \n",
    "tweets_all['date'] = tweets_all['date'].dt.strftime(\"%b %d, %Y\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data type of the date column\n",
    "print(tweets_all['date'].dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the excel file to be loaded in by R "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Ryan\\Coding Projects\\Twitter Data Scraping\\\\\"\n",
    "\n",
    "tweets_all.to_excel(path + 'master_tweets_2021.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7959011f7c65b6277518dac78edd11ca4ca331adbf641f303d55a67ad931e17d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
