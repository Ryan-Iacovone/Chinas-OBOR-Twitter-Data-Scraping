```{r}
library(tidyverse)
library(readxl)
library(tidytext)
library(ggthemes)
library(udpipe) #library for lemmatization 
library(cowplot) 

#Gathering a list of all the excel files from the current directory (assuming that only excel files with tweet data are in this file )
tweet_file_list1421 <- list.files(pattern = "xlsx")

df_list <- lapply(tweet_file_list1421, read_excel)

# Use do.call and rbind to combine all data frames in the list
combined1421 <- do.call(rbind, df_list)
```
#Preprocess and clean data


```{r}
#Need to do some brief investigations on the earlier year datsets because lack of tweets
#tweet_words_china %>% filter(year == 2014)
```


#also will want to set up custom stop words to filter out the hashtags I'm using?

#Initaizling the training data with just 2018
```{r eval=FALSE, include=FALSE}
#changing the format of the ID string to display all numbers 
tweets_master$id <- format(as.character(tweets_master$id), width = 20, scientific = FALSE)

#Changing the date variable to a date format because I don't care about the time of tweet's time of day 
tweets_master$date <- as.Date(tweets_master$date, format = '%b %d, %Y')

#grabbing the month of the tweet tweeted for later analysis to group by month
tweets_master$month <- format(tweets_master$date, "%m-%y") 

#Grabbing only the tweets that the python program recognized as english
tweets_EN <- tweets_master %>% filter(language == "en")
```

#making the combined data for all the tweet years
```{r}
rm(df_list)

#changing the format of the ID string to display all numbers 
combined1421$id <- format(as.character(combined1421$id), width = 20, scientific = FALSE)

#Changing the date variable to a date format because I don't care about the time of tweet's time of day 
combined1421$date <- as.Date(combined1421$date, format = '%b %d, %Y')

#grabbing the month and year (12-18 for december 2018) of the tweet tweeted for later analysis to group by month and year 
combined1421$month <- format(combined1421$date, "%m-%y") 

#grabbing just the year of the tweet tweeted for later analysis to group by year 
combined1421$year <- format(combined1421$date, "%Y") 

#Grabbing only the tweets that the python program recognized as english
tweets_EN <- combined1421 %>% filter(language == "en") 
```


```{r eval=FALSE, include=FALSE}
#Testing out preprocessing code on 1 sentence, sentence 9

#>fsdf

i=9
#Contents of tweet i
tweets_master$content[i]

#this is the base coded we will build on to properly extract our tweet info using unnest_tokens() function from tidytext library
tweets_master[i,] %>% 
  unnest_tokens(word, content) %>%
  select(word)

#Note that the unnest_tokens function tries to convert tokens into words and strips characters important to twitter such as # and @. A token in twitter is not the same as in regular English. For this reason, instead of using the default token, words, we define a regex that captures twitter character. The pattern appears complex but all we are defining is a patter that starts with @, # or neither and is followed by any combination of letters or digits:

pattern <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"

tweets_master[i,] %>%      
  
  #Removes links to pictures/videos from the tweet:
  mutate(content = str_replace_all(content, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
  
  #We now use the unnest_tokens() function with the regex pattern above to see hashtags and mentions:
  unnest_tokens(word, content, token = "regex", pattern = pattern) %>% 
  select(word)
```


```{r eval=FALSE, include=FALSE}
#implementing the core of the preprocessing code on all of our tweets #################Core preprocessing Code#####################


tweet_words_china1 <- tweets_master %>% 
  mutate(content = str_replace_all(content, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
  unnest_tokens(word, content, token = "regex", pattern = pattern)
```


```{r eval=FALSE, include=FALSE}
#Steps to improving the core code above:

#Filtering out tweets that include 'stop words' to gather better insight into the most relevant words tweeted

tweet_words_china2 <- tweets_master %>% 
  mutate(content = str_replace_all(content, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
  unnest_tokens(word, content, token = "regex", pattern = pattern) %>%
  filter(!word %in% stop_words$word)
```


```{r}
#Note that the unnest_tokens function tries to convert tokens into words and strips characters important to twitter such as # and @. A token in twitter is not the same as in regular English. For this reason, instead of using the default token, words, we define a regex that captures twitter character. The pattern appears complex but all we are defining is a patter that starts with @, # or neither and is followed by any combination of letters or digits:

pattern <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"

#Some exploration of the resulting words (not show here) reveals a couple of unwanted characteristics in our tokens. First, some of our tokens are just numbers (years for example). We want to remove these and we can find them using the regex ^\d+$. Second, some of our tokens come from a quote and they start with '. We want to remove the ' when it's at the start of a word, so we will use str_replace(). We add these two lines to the code above to generate our final table:
#We use only our English tweets for creating the tweet_words_china dataframe

tweet_words_china <- tweets_EN %>% 
  mutate(content = str_replace_all(content, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
  unnest_tokens(word, content, token = "regex", pattern = pattern) %>%
  filter(!word %in% stop_words$word &
           !str_detect(word, "^\\d+$")) %>%
  mutate(word = str_replace(word, "^'", ""))
```

###################ONLY RUN THIS CODE AGAIN IF WE NEED TO CREATE ANOTHER lemmatization_core.RDA FILE TAKES A LITTLE BIT TO RUN
```{r eval=FALSE, include=FALSE}
#further preprocessing steps by lemmatizing the variable word  

#Creating a temporary copy of our tweet_words dataframe to  add a temporary column 
fake_TWC <- tweet_words_china

#Creating a new word column that eliminates all the # and @ signs if they're in the word variable to then pass to our lemmatization function
fake_TWC$word_nohash <- gsub("[#@]", "", fake_TWC$word)

#Loops through all the words in our variable word then returns the simpliest form of that word among other cool stuff like part of speech
lemmatization_all <- udpipe(fake_TWC$word_nohash, object = 'english')

#Code to get rid of the 's lemmatizations (it would try to turn a word like "father's" into "father" and " 's " but obviously we only care about the word)
lemmatization_core <- lemmatization_all %>% filter(!(xpos %in% c("POS", "PRP")))

#removing the lemmatization_all and fake_TWC data frames since they are not longer useful 
rm(lemmatization_all, fake_TWC)

#Code to write an r data file (rda) 
save(lemmatization_core, file = "C:/Users/Ryan/Coding Projects/Twitter Data Scraping/lemmatization_core.rda")

#Code to then load that RDA file back in 
load("C:/Users/Ryan/Coding Projects/Twitter Data Scraping/lemmatization_core.rda")
```


```{r}
file_term <- "lemmatization"
directory <- "C:/Users/Ryan/Coding Projects/Twitter Data Scraping"

# Loop through all files in the directory to search for an older version of this document (file_name)
# If it exists, load the file
for (filename in list.files(directory)) {
  
  #Searches through all the filenames trying to match our file_term to the filename and once it does we then load that file path  
  if (grepl(file_term, filename)) {
    
    #Combines the directory and name of file we've matched to create a file path so we can load in the RDA file 
    file_pathy <- file.path(directory, filename)
    
    #loads the lemitization dataframe we created previously 
    load(file_pathy)
    
    #exits the loop once we've found the file with "lemmatization" in it 
    break
  }  
}    
```


```{r}
#creates a new character vector based on 2 variables that contain words before and after they've gone through lemmitization   
word <- lemmatization_core$sentence #the original vector of characters as they appeared in the tweets 
lemma <- lemmatization_core$lemma #The lemmitized version of each word now ready for sentiment analysis  

#Create a dictionary as the combined inputs of the two vectors above "word" and "lemma" so their values are now connected via the key of the original word
lemma_dict <- setNames(lemma, word)

#Code to search through the lemma_dict for a specific word to be lemmitized  
word_to_lookup <- "opportunities"

ifelse(word_to_lookup %in% names(lemma_dict), lemma_dict[[word_to_lookup]], NA)
```


```{r eval=FALSE, include=FALSE}
#This is an example of when not to run a loop function with sapply in R because it takes way too long to run
#define our function to loops through all the words in tweet_words_china to then return the lemmatized version of the word as a new variable

to_lemmatize <- function(x) {
  
  # create a new variable with all values equal to 0 then rewriting over it for each new variable
  
  word_to_lookup <- x
  
  ifelse(word_to_lookup %in% names(lemma_dict), lemma_dict[[word_to_lookup]], NA)
}

#This specifically takes long time to run
tweet_words_china$word_lems <- sapply(tweet_words_china$word, FUN = to_lemmatize)

#Need to change the names of the columns in the tweet_words_china dataframe to be able to merge on "word" during sentiment analysis 

names(tweet_words_china)[names(tweet_words_china) == "word"] <- "orig_word"
names(tweet_words_china)[names(tweet_words_china) == "word_lems"] <- "word"
```


```{r}
#Going through the list of our original words then using the match function to connect them with their lemmitized version 

# create a vector of original words to lemmatize, ONLY RUN THIS CODE ONCE OTHERWISE IT CREATES MORE VARIABLES!!!!!!!
orig_words <- tweet_words_china$word

# use the match function to replace the original words with their lemmas
lemmatized_words <- lemma_dict[match(orig_words, names(lemma_dict))]

# replace any missing lemmas with the original words, idk if I want to do this
#lemmatized_words[is.na(lemmatized_words)] <- orig_words[is.na(lemmatized_words)]

# add the lemmatized words to the dataframe, ONLY RUN THIS CODE ONCE OTHERWISE IT CREATES MORE VARIABLES!!!!!!!
tweet_words_china$word_lems <- lemmatized_words

# rename the columns
names(tweet_words_china)[names(tweet_words_china) == "word"] <- "orig_word"
names(tweet_words_china)[names(tweet_words_china) == "word_lems"] <- "word"
```

#Cool code for extracting certain parts of the lemmitization table 
```{r eval=FALSE, include=FALSE}
# Load a pre-trained model for English
model <- udpipe_download_model(language = "english")
udmodel <- udpipe_load_model(file = model$file_model)

# Define a text to annotate
text <- "I am happy to be learning natural language processing with udpipe in R."

# Perform POS tagging and dependency parsing on the text
doc <- udpipe_annotate(udmodel, tweet_words_china$word)

doc <- as.data.frame(doc)

doc1 <- doc %>% filter(!(xpos %in% c("POS", "PRP")))

# Extract the POS tags and lemmas from the annotated text
pos_tags <- as.data.frame(doc)[,c("token_id", "upos")]
lemmas <- as.data.frame(doc)[,c("token_id", "lemma")]

# Print the POS tags and lemmas
print(pos_tags)
print(lemmas)

```


#Gaining insight into the most common words that were tweeted ensuring there's a difference between the original word and then lemmitized version
```{r}
# Most common words that appear in our orig_words list (includes # and @)

tweet_words_china %>% 
  count(orig_word) %>%
  arrange(desc(n))

#code that achieves the same result as above
#tweet_words_china %>% count(word, sort = TRUE)

#Most common words after lemmitization has occurred using the 'word' list
tweet_words_china %>% 
  count(word) %>%
  top_n(15, n) %>%
  mutate(word = reorder(word, n)) %>%
  arrange(desc(n))
```

###################################INVESTIGATING WEIRD WORDS AREA##############################################
```{r eval=FALSE, include=FALSE}
bad_word = "@cpec"

grr_lem <- tweet_words_china %>% filter(orig_word == bad_word)

humm_TWC <- tweet_words_china %>% filter(word == bad_word) 


rm(grr_lem, humm_TWC)
```

#Gaining insight into the twitter accounts that tweeted most often about this topic to ward off potential bots and see how the big players influcing convo
```{r}
#code that gathers the top 5 and 10 users that we tweeted about China's BRI
top_10_df <- tweets_EN %>% count(username, sort = TRUE) %>% top_n(10)
top_5_df <- tweets_EN %>% count(username, sort = TRUE) %>% top_n(5)

#Code for separate sentiment analysis just on the top tweeters to see how they are influencing the conversation

#taking the top 10 and top 5 tweeters and putting them into a list
top_ten <- top_10_df$username
top_five <- top_5_df$username
```

###################################INVESTIGATING WEIRD ACCOUNTS AREA##############################################
```{r eval=FALSE, include=FALSE}

bad_account = "@cpec"

grr_lem <- tweet_words_china %>% filter(orig_word == bad_word)

humm_TWC <- tweet_words_china %>% filter(word == bad_word) 


rm(grr_lem, humm_TWC)
```


###### Perform sentiment analysis ###### 

#Afinn lexicon 
```{r}
#In sentiment analysis we assign a word to one or more "sentiment". Although this approach will miss context dependent sentiments, such as sarcasm, when performed on large numbers of words, summaries can provide insights.

#The first step in sentiment analysis is to assign a sentiment to each word. The tidytext package includes several maps or lexicons in the object sentiments:

#For this sentiment analysis I will be using the 'AFINN' lexicon because of it's number range although there are others I could use like "bing", "nrc", or "loughran". Read this help file: ?sentiments

#The AFINN lexicon assigns a score between -5 and 5, with -5 the most negative and 5 the most positive.
afinn <- get_sentiments("afinn")

#exploring afinn dataset
afinn %>% filter(value == 4)

#Code to check what words are in the afinn database
afinn %>% filter(word == 'prosper')
```


```{r}
#We can combine the words and sentiments using inner_join(), which will only keep words associated with a sentiment.
afinn_scores_b <- tweet_words_china %>% inner_join(afinn, by = "word")
```

#adding a sentiment column in the afinn_scores dataset to just look at positive and negative words based on the numbering system
```{r}
afinn_scores_b$sentiment <- ifelse(str_detect(afinn_scores_b$value, "-"), "negative", "positive")
```

```{r}
#Getting rid of baised chinese state media sources from overall view 
china_offical_news <- c("CCTV",  "ChinaDaily", "beltroadnews", "XHNews", "China__Focus", "ChinaEUMission", "CGTNOfficial", "globaltimesnews", "PDChina", "ChinaDailyWorld")

afinn_scores <- afinn_scores_b %>% filter(!username %in% china_offical_news)
```


```{r}
#investigating the top tweeters that had a word match our lexicon and therefore would be more impactful in calculating actual sentiment 
#note that we're counting words (that are in the lexicon) per username, not tweets 

afinn_scores %>% count(username, sort = TRUE) %>% top_n(20)
```

#Most common word analyis and their assocaited sentiment
```{r}
#List of most common words and their associated sentiment value, filtering out CPEC_Official because their tweets were useless 

afinn_scores %>%
  filter(username != "CPEC_Official") %>%
  group_by(word) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  left_join(afinn_scores %>% distinct(word, value), by = "word")


afinn_word_count <- afinn_scores %>%
  filter(username != "CPEC_Official") %>% 
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
```


```{r}
#List of most common positively sentiment words

afinn_scores %>%
  filter(username != "CPEC_Official" & !str_detect(value, "-")) %>%
  group_by(word) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  left_join(afinn_scores %>% distinct(word, value), by = "word")

```


```{r}
#List of most common negatively sentiment words 

afinn_scores %>%
  filter(username != "CPEC_Official" & str_detect(value, "-")) %>%
  group_by(word) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  left_join(afinn_scores %>% distinct(word, value), by = "word")

```


#Concise bar chart that nicely plots top negative and positve words from afinn lexicon
```{r}
afinn_bar <- afinn_word_count %>% group_by(sentiment) %>% slice_max(n, n = 10) %>% ungroup() %>% mutate(word = reorder(word, n))

afinn_bar %>% ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL) +
  theme_clean()
```





#Visualizations section:

Group by data manipulation then specific graph, rinse and repeat 
Ideas:

#some idea for visualizations would be to plot sentiment of China's BRI over time then see if negative sentiment corresponds with major political events

###### Afinn visualtions ###### 

#Prepping our the data visualization graph 
```{r}
#Creating a dataframe called tweet_sentiments that contains the month, average sentiment of tweets for that month, and number of tweets

#n_tweets = n() is used to create a new column in the summarized data frame that contains the number of tweets per month.
#When using dplyr functions like summarize(), you can use the n() function to count the number of rows in a group. In this case, we're grouping the data by month (in the format month-year) using group_by(month), and then counting the number of rows in each group (i.e., the number of tweets in each month) using n()

tweet_sentiments <- afinn_scores %>% 
  group_by(month, year) %>% 
  summarise(sentiment = mean(value), n_tweets = n())


#Changing the format of the month variable back into a date format so that we can use the geom_smooth function with it
tweet_sentiments <- tweet_sentiments %>%
  mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))


#Ensuring that the correct number of tweets are gathered for tweet_sentiments above 
#tweets_master %>% filter(month == '07') 

#afinn_scores %>% filter(month == '07') 
```

```{r}
#Code to look at all the years I've collected data for as a whole on a single graph (minus chinese state media sources)

tweet_sentiments %>% ggplot(aes(month, sentiment)) +
  
  #geom_smooth(color = "red", linetype = "solid", se = FALSE) +
  geom_line(color = "blue", size = 1) +
  geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue") +
  labs(
    title = "Average Sentiment Score of Tweets by Month from 2014 - 2021", 
    x = "Month", 
    y = "Sentiment Score (-6 to 6)",
    size = "Tweet Size"
    )+
  
  #scale_y_continuous(
    #breaks = seq(-1, 1, by = .2),
    #limits = c(-1, 1)) +
  
  #Organizes the x axis by each month then formats how we want it labeled 
   scale_x_date(
    date_breaks = "1 year",
    date_labels = "%b '%y") +
    #date_minor_breaks = "1 month", minor_breaks = "1 month") +
  
  theme_clean() +
  
  theme(
    panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
    
    legend.position = c(.7, .2),
    legend.direction = "horizontal",
    legend.title = element_text(size = 10.5), 
    legend.text = element_text(size = 10) 
    ) 
```


```{r}
##################Code to itterate over each year of tweet analysis #################################

sentiment_year <- list()

for (yr in unique(tweet_sentiments$year)) {
  df_year <- subset(tweet_sentiments, year == yr) 
  
  plot <- ggplot(df_year, aes(month, sentiment)) +
  
  #geom_smooth(color = "red", linetype = "solid", se = FALSE) +
  geom_line(color = "blue", size = 1) +
  geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue") +
  labs(
    title = paste0("Average Sentiment Score of Tweets by Month in ", yr), 
    x = paste0("Month (", yr, ")"), 
    y = "Sentiment Score (-6 to 6)",
    size = "Tweet Size"
    )+
  
  #scale_y_continuous(
    #breaks = seq(-1, 1, by = .2),
    #limits = c(-1, 1)) +
  
  #Organizes the x axis by each month then formats how we want it labeled 
   scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b") +
    #date_minor_breaks = "1 month", minor_breaks = "1 month") +
  
  theme_clean() +
  
  theme(
    panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
    
    legend.position = c(.7, .3),
    legend.direction = "horizontal",
    legend.title = element_text(size = 10.5), 
    legend.text = element_text(size = 10) 
    ) 
  
  sentiment_year[[yr]] <- plot
    
}

sentiment_year[["2020"]]


#plot_grid(plotlist = sentiment_year, ncol = 3)
```


#Investigating twitter accounts that who's tweets contain sentiments that match our lexicon list and therefore are more influential in the data 
```{r}
#code that gathers the top 5 and 10 users that we tweeted about China's BRI
top_10_afinn <- afinn_scores %>% count(username, sort = TRUE) %>% top_n(10)
top_5_afinn <- afinn_scores %>% count(username, sort = TRUE) %>% top_n(5)

#Code for separate sentiment analysis just on the top tweeters to see how they are influencing the conversation

#taking the top 10 and top 5 tweeters and putting them into a list
top_ten_a <- top_10_afinn$username
top_five_a <- top_5_afinn$username
```


```{r}
#Looking at the sentiment of our top 10 tweeters (gathered from code above) to see how they are influencing the conversation

#Note that the the top tweeters does not necessarily translate to the top tweeters who displayed some sentiment about belt and Road 
#hence, the drastic decline of user "Shyam17"

sentiments_top10 <- afinn_scores %>% 
  filter(username %in% top_ten) %>% 
  group_by(month) %>% 
  summarise(sentiment = mean(value), n_tweets = n())

#verifiying that only the top 10 usernames saved 
#sentiments_top10 %>% count(username, sort = TRUE)


#Changing the format of the month variable back into a date format so that we can use the geom_smooth function with it
sentiments_top10 <- sentiments_top10 %>%
  mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))
```


```{r}
sentiments_top10 %>% ggplot(aes(month, sentiment)) +
  geom_line(color = "blue", size = 1) +
  geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue") +
  
  labs(
    title = "Average Sentiment Score by Top 10 Tweeters in 2018", 
    x = "Month", 
    y = "Sentiment Score (-6 to 6)",
    size = "Tweet Size"
    )+
  
  #scale_y_continuous(
    #breaks = seq(-.8, 1.2, by = .2),
    #limits = c(-.8, 1.2)) +
  
  #Organizes the x axis by each month then formats how we want it labeled 
   scale_x_date(
    date_breaks = "1 year",
    date_labels = "%b '%y") +
    #date_minor_breaks = "1 month", minor_breaks = "1 month") +
  
  theme_clean() +
  
  theme(
    panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
    
    legend.position = c(.6, .8),
    legend.direction = "horizontal",
    legend.title = element_text(size = 10.5), 
    legend.text = element_text(size = 10) 
    ) 
```


```{r}
#Looking at the sentiment of our top 5 tweeters to see how they are influencing the conversation
#Noting the the top tweeters does not necessarily translate to the top tweeters who displayed some sentiment about belt and Roaod 
#hence, the drastic decline of user "Shyam17"

sentiments_top5 <- afinn_scores %>% 
  filter(username %in% top_five) %>% 
  group_by(month) %>% 
  summarise(sentiment = mean(value), n_tweets = n())

#verifiying that only the top 10 usernames saved 
#sentiments_top5 %>% count(username, sort = TRUE)


#Changing the format of the month variable back into a date format so that we can use the geom_smooth function with it
sentiments_top5 <- sentiments_top5 %>%
  mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))
```



```{r eval=FALSE, include=FALSE}
###############REPREATED CODE ##################

#code that gathers the top 5 and 10 users that we tweeted about China's BRI
top_10_afinn <- afinn_scores %>% count(username, sort = TRUE) %>% top_n(10)
top_5_afinn <- afinn_scores %>% count(username, sort = TRUE) %>% top_n(5)

#Code for separate sentiment analysis just on the top tweeters to see how they are influencing the conversation

#taking the top 10 and top 5 tweeters and putting them into a list
top_ten_a <- top_10_afinn$username
top_five_a <- top_5_afinn$username
```

######REFINE THE CODE BELOW TO MORE EFFICEINTLY SORT BY USERNAME BY SUBSETTING LIKE ABOVE AND USING THE PASTE0 FUNCTION TO CANCATINATE STRINGS

```{r}
top_users_list <- list()

#Creating 10 different dataframes each based on 1 username 

for (user in top_ten_a) {
  hum <- afinn_scores %>% filter(username == user) %>%
    group_by(month) %>% 
    summarise(sentiment = mean(value), n_tweets = n())
  
  hum <- hum %>%
    mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))
  
  
  top_users_list[[user]] <- hum 
  rm(hum)
}

top_users_list[["beltandroad1"]]
```

```{r}
#Code to itterate through the top 10 tweeters to create the graphs 

top10_user_plots <- list()

# Loop through time data frames and create plots for the factor varaible teeth 
for (user in top_ten_a) { #Equivalent of "range(len(dfs_times))" in python 
  
  plot <- ggplot(userdf[[user]], aes(month, sentiment)) +
    geom_line(color = "red", size = 1) +
    geom_point(aes(size = n_tweets), alpha = 0.6, color = "red") +
    
    labs(
    title = user , 
    x = "Month", 
    y = "Sentiment Score (-6 to 6)") +
    
    #Organizes the x axis by each month then formats how we want it labeled 
    scale_x_date(
      date_breaks = "1 year",
      date_labels = "%b '%y") +
    
    theme_clean()
    
    top10_user_plots[[user]] <- plot
    
}

names(top10_user_plots)

top10_user_plots[["WongMNC_CtrExDr"]]

plot_grid(plotlist = top10_user_plots, ncol = 4)
```

#investingating the sentiment of "china state-affiliated media" or "China government organization" designation by twitter

```{r}
#taking all Chinese state media twitter accounts and putting them in a list 
china_offical_news <- c("CCTV",  "ChinaDaily", "beltroadnews", "XHNews", "China__Focus", "ChinaEUMission", "CGTNOfficial", "globaltimesnews", "PDChina", "ChinaDailyWorld")
```

```{r}
#Looking at the sentiment of Chinese state media twitter accounts to see how they are influencing the conversation

#Note that the the top tweeters does not necessarily translate to the top tweeters who displayed some sentiment about belt and Road 
#hence, the drastic decline of user "Shyam17"

sentiments_CSM <- afinn_scores_b %>% 
  filter(username %in% china_offical_news) %>% 
  group_by(month) %>% 
  summarise(sentiment = mean(value), n_tweets = n())

#verifiying that only the top 10 usernames saved 
#sentiments_top10 %>% count(username, sort = TRUE)


#Changing the format of the month variable back into a date format so that we can use the geom_smooth function with it
sentiments_CSM <- sentiments_CSM %>%
  mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))
```


```{r}
sentiments_CSM %>% ggplot(aes(month, sentiment)) +
  geom_line(color = "blue", size = 1) +
  geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue") +
  
  labs(
    title = "Average Sentiment Score by Offical Chinese State Media", 
    x = "Month", 
    y = "Sentiment Score (-6 to 6)",
    size = "Tweet Size"
    )+
  
  #scale_y_continuous(
    #breaks = seq(-.8, 1.2, by = .2),
    #limits = c(-.8, 1.2)) +
  
  #Organizes the x axis by each month then formats how we want it labeled 
   scale_x_date(
    date_breaks = "1 year",
    date_labels = "%b '%y") +
    #date_minor_breaks = "1 month", minor_breaks = "1 month") +
  
  theme_clean() +
  
  theme(
    panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
    
    legend.position = c(.6, .8),
    legend.direction = "horizontal",
    legend.title = element_text(size = 10.5), 
    legend.text = element_text(size = 10) 
    ) 
```


```{r}
#combined plot - all tweeters vs top 10 tweeters vs top 5 tweeters vs Chinese state media 
  
#Changing the format of the month variable back into a date format so that we can use the geom_smooth function with it
tweet_sentiments <- tweet_sentiments %>%
  mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))  

#Graph plotting Top 5, top 10, and all tweets together on the same plot to see their influnce 
ggplot() +
  geom_point(data = tweet_sentiments, aes(month, sentiment, color = "tweet_sentiments"), size = 3) +
  geom_smooth(data = tweet_sentiments, aes(month, sentiment), se = FALSE, color = "red")+

  geom_point(data = sentiments_top10, aes(month, sentiment, color = "sentiments_top10"), size = 3) +
  geom_smooth(data = sentiments_top10, aes(month, sentiment), se = FALSE, color = "blue") +
  
  geom_point(data = sentiments_top5, aes(month, sentiment, color = "sentiments_top5"), size = 3) +
  geom_smooth(data = sentiments_top5, aes(month, sentiment), se = FALSE, color = "green") +
  
  geom_point(data = sentiments_CSM, aes(month, sentiment, color = "sentiments_CSM"), size = 3) +
  geom_smooth(data = sentiments_CSM, aes(month, sentiment), se = FALSE, color = "orange") +
  
  scale_color_manual(values = c("tweet_sentiments" = "red", "sentiments_top10" = "blue",  "sentiments_top5" = "green", "sentiments_CSM" = "orange"), 
                     labels = c("Top Ten", "Top Five", "All Tweeters")) +
  labs(
    title = "Average Sentiment Score by Month from 2014-2021", 
    x = "Month", 
    y = "Sentiment Score (-6 to 6)",
    color = "Dataframe") +
  
  scale_y_continuous(
    breaks = seq(-1, 2, by = .2),
    limits = c(-1, 2)) +
  
  #Organizes the x axis by each month then formats how we want it labeled 
   scale_x_date(
    date_breaks = "1 year",
    date_labels = "%b '%y") +
  
  theme_clean()
```



###### Bing visualtions ######


```{r}
#Present results


```

