```{r}
#Reading in all the libraries and wrangled data frames from this initial R file 
source("C:/Users/Ryan/Coding Projects/Twitter Data Scraping/sent_wrangling.R")
```

# Sentiment Analysis 


## Afinn lexicon

### Prerpocessing Afinn lexicon with our own data 
```{r}
#In sentiment analysis we assign a word to one or more "sentiment". Although this approach will miss context dependent sentiments, such as sarcasm, when performed on large numbers of words, summaries can provide insights.

#The first step in sentiment analysis is to assign a sentiment to each word. The tidytext package includes several maps or lexicons in the object sentiments:

#For this sentiment analysis I will be using the 'AFINN' lexicon because of it's number range although there are others I could use like "bing", "nrc", or "loughran". Read this help file: ?sentiments

#The AFINN lexicon assigns a score between -5 and 5, with -5 the most negative and 5 the most positive.
afinn <- get_sentiments("afinn")

#exploring afinn dataset
afinn %>% filter(value == 4)

#Code to check what words are in the afinn database
afinn %>% filter(word == 'prosper')
```


```{r}
#We can combine the sentiments and words from out china databse using inner_join(), which will only keep words associated with a sentiment.
#"afinn_scores_b" means that the dataset is biased because it still contains chinese affiliated news outlets 

afinn_scores_b <- tweet_words_china %>% inner_join(afinn, by = "word")
```


#### tidying up our affin data frame
```{r}
rm(afinn)

#adding a sentiment column in the afinn_scores dataset to just look at positive and negative words based on the numbering system
afinn_scores_b$sentiment <- ifelse(str_detect(afinn_scores_b$value, "-"), "negative", "positive")

#With the new sentiment column we're standardizing the values for comparison against the bing lexicon 
afinn_scores_b$stan <- ifelse(afinn_scores_b$sentiment == "positive", 1, -1)
```


#### Getting rid of biased Chinese state media sources from overall afinn_scores data frame 
```{r}
#official Chinese news sites were gathered by looking up common chinese state media and see if they showed up in our dataset. This was then confirmed by checking each account in question and seeing if it had the "chinese state media" banner on their twitter profiles

china_offical_news <- c("CCTV",  "ChinaDaily", "beltroadnews", "XHNews", "China__Focus", "ChinaEUMission", "CGTNOfficial", "globaltimesnews", "PDChina", "ChinaDailyWorld", "chinafrica1")

afinn_scores <- afinn_scores_b %>% filter(!username %in% china_offical_news)
```



### Investigating/analysis (no graphs) on Afinn lexicon with our own data 


#### Investigating the top tweeters that had a word match our lexicon and therefore will be more impact in calculating actual sentiment 
```{r}
#note that we're counting words (that are in the lexicon) per username, not tweets 

afinn_scores %>% count(username, sort = TRUE) %>% top_n(20)
```


####List of most common words and their associated sentiment value, filtering out CPEC_Official because their tweets were useless 
```{r}
#Both chunks of code achieve the same results

afinn_scores %>%
  filter(username != "CPEC_Official") %>%
  group_by(word) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  left_join(afinn_scores %>% distinct(word, value), by = "word")

#We save our top words and their associated sentiments into "afinn_word_count" for later analysis with bar charting the top negative and positive words visually
afinn_word_count <- afinn_scores %>%
  filter(username != "CPEC_Official") %>% 
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
```


#### List of most common positive and negative sentiment words
```{r}
# Positive 

afinn_scores %>%
  filter(username != "CPEC_Official" & !str_detect(value, "-")) %>%
  group_by(word) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  left_join(afinn_scores %>% distinct(word, value), by = "word")


# Negative 

afinn_scores %>%
  filter(username != "CPEC_Official" & str_detect(value, "-")) %>%
  group_by(word) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  left_join(afinn_scores %>% distinct(word, value), by = "word")
```


##### Concise bar chart that nicely plots top negative and positive words from afinn lexicon
```{r}
afinn_bar <- afinn_word_count %>% group_by(sentiment) %>% slice_max(n, n = 10) %>% ungroup() %>% mutate(word = reorder(word, n))

#cleaning up the global environment
rm(afinn_word_count)

afinn_bar %>% ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  
  facet_wrap(~sentiment, scales = "free_y") +
  
  labs(x = "Contribution to sentiment",
       y = NULL) +
  
  theme_clean()
```



# Visualizations section of sentiment analysis - Code organized by data manipulation then specific graph, rinse and repeat 
    

## Afinn visualizations


###  Prepping afinn_scores for visualization of sentiment over all years in dataset
```{r}
#Creating a dataframe called tweet_sentiments that contains the month, year, average sentiment of tweets for timer period, and number of tweets

#n_tweets = n() is used to create a new column in the summarized data frame that contains the number of tweets per month.
#When using dplyr functions like summarize(), you can use the n() function to count the number of rows in a group. In this case, we're grouping the data by month (in the format month-year) using group_by(month), and then counting the number of rows in each group (i.e., the number of tweets in each month) using n()

tweet_sentiments_a <- afinn_scores %>% 
  group_by(month, year) %>% 
  summarise(sentiment = mean(value), n_tweets = n()) %>%
  #Changing the format of the month variable back into a date format so that we can use the geom_smooth function with it
  mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))


#Ensuring that the correct number of tweets are gathered for tweet_sentiments above 
#tweets_EN %>% filter(month == '10-14') 

#afinn_scores %>% filter(month == '10-14') 
```


### Code to look at average sntiment by month from 2014-2023 on a single graph (minus Chinese state media sources)
```{r}
tweet_sentiments_a %>% ggplot(aes(month, sentiment)) +
  
  #geom_smooth(color = "red", linetype = "solid", se = FALSE) +
  geom_line(color = "blue", size = 1) +
  geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue") +
  
  # Add vertical lines for each year (like x = 3)
  geom_vline(aes(xintercept = as.numeric(as.Date(paste0(year, "-01-01")))), 
             color = "gray", linetype = "dotted") +            
  labs(
    title = "Average Sentiment Score of Tweets by Month from 2014 - 2023", 
    x = "Time", 
    y = "Sentiment Score (-6 to 6)",
    size = "Tweet Size"
    )+
  
  #scale_y_continuous(
    #breaks = seq(-1, 1, by = .2),
    #limits = c(-1, 1)) +
  
  #Organizes the x axis by each month then formats how we want it labeled 
   scale_x_date(
    date_breaks = "1 year",
    date_labels = "%b '%y",
    expand = c(0.1, 1)) +
    #date_minor_breaks = "1 month", minor_breaks = "1 month") +
  
  theme_clean() +
  
  theme(
    panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
    
    legend.position = c(.7, .8),
    legend.direction = "horizontal",
    legend.title = element_text(size = 10.5), 
    legend.text = element_text(size = 10) 
    ) 
```


### Code to create a graph for each year of sentiment analysis (specific year analysis)
```{r}
sentiment_year <- list()

for (yr in unique(tweet_sentiments_a$year)) {
  df_year <- subset(tweet_sentiments_a, year == yr) 
  
  plot <- ggplot(df_year, aes(month, sentiment)) +
  
  #geom_smooth(color = "red", linetype = "solid", se = FALSE) +
  geom_line(color = "blue", size = 1) +
  geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue") +
    
  # Add vertical lines for each month
  geom_vline(aes(xintercept = month), color = "gray", linetype = "dotted") +

  labs(
    title = paste0("Average Sentiment Score of Tweets by Month in ", yr), 
    x = paste0("Month (", yr, ")"), 
    y = "Sentiment Score (-6 to 6)",
    size = "Tweet Size"
    )+
  
  #scale_y_continuous(
    #breaks = seq(-1, 1, by = .2),
    #limits = c(-1, 1)) +
  
  #Organizes the x axis by each month then formats how we want it labeled 
   scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b") +
    #date_minor_breaks = "1 month", minor_breaks = "1 month") +
  
  theme_clean() +
  
  theme(
    panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
    
    legend.position = c(.7, .8),
    legend.direction = "horizontal",
    legend.title = element_text(size = 10.5), 
    legend.text = element_text(size = 10) 
    ) 
  
  sentiment_year[[yr]] <- plot
  
  rm(plot, df_year)
    
}

sentiment_year[["2018"]]


#plot_grid(plotlist = sentiment_year, ncol = 3)
```


### Sentiment Analysis of the top 5, 10, and 20 twitter accounts posting about China's BRI (group)
```{r}
#Investigating top 5, 10, and 20 twitter accounts who's tweets contain sentiments that match our lexicon list and therefore are more influential in the data 

#code that gathers the top 5, 10, and 20 users that tweeted about China's BRI and creates a temporary data frame 
top_20_afinn <- afinn_scores %>% count(username, sort = TRUE) %>% top_n(20)
top_10_afinn <- afinn_scores %>% count(username, sort = TRUE) %>% top_n(10)
top_5_afinn <- afinn_scores %>% count(username, sort = TRUE) %>% top_n(5)

#Code for separate sentiment analysis just on the top tweeters to see how they are influencing the conversation

#Extracting the distinct usernames from the top 5, 10, and 20 df's respectively and putting the usernames into a list
top_20_a <- top_20_afinn$username
top_10_a <- top_10_afinn$username
top_5_a <- top_5_afinn$username

#tidying up 
rm(top_10_afinn, top_5_afinn, top_20_afinn)
```


```{r}
#Looking at the sentiment of our top 10 tweeters (gathered from code above) to see how they are influencing the conversation

#Note that the the top tweeters does not necessarily translate to the top tweeters who displayed some sentiment about belt and Road 
#hence, the drastic decline of user "Shyam17"

O_list <- list(top_5_a, top_10_a, top_20_a)
names(O_list) <- c("top_5", "top_10", "top_20")

TopX_list <- list()

for (list in names(O_list)) {
  topx <- afinn_scores %>% 
  filter(username %in% O_list[[list]]) %>% 
  group_by(month) %>% 
  summarise(sentiment = mean(value), n_tweets = n()) %>%
  mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))
  
  TopX_list[[list]] <- topx
} 

TopX_list[["top_10"]]

rm(O_list, topx, list)  

#verifiying that only the top 10 usernames saved 
#sentiments_top10 %>% count(username, sort = TRUE)
```


#### Graphing top 5, 10, and 20 twitter accounts sentiment individually
```{r}
#Graphing top 5, 10, and 20 twitter accounts sentiment individually

topx_graphs <- list()

for (topx in names(TopX_list)) {

plot <-ggplot(TopX_list[[topx]], aes(month, sentiment)) +
  
  geom_line(color = "blue", size = 1) +
  geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue") +
  
  labs(
    title = paste0("Average Sentiment Score by ", topx ," Tweeters over time"), 
    x = "Month", 
    y = "Sentiment Score (-6 to 6)",
    size = "Tweet Size"
    )+
  
  #scale_y_continuous(
    #breaks = seq(-.8, 1.2, by = .2),
    #limits = c(-.8, 1.2)) +
  
  #Organizes the x axis by each month then formats how we want it labeled 
   scale_x_date(
    date_breaks = "1 year",
    date_labels = "%b '%y") +
    #date_minor_breaks = "1 month", minor_breaks = "1 month") +
  
  theme_clean() +
  
  theme(
    panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
    
    legend.position = c(.6, .8),
    legend.direction = "horizontal",
    legend.title = element_text(size = 10.5), 
    legend.text = element_text(size = 10) 
    ) 
  
  topx_graphs[[topx]] <- plot
}

topx_graphs[["top_10"]]

rm(plot, topx)
```


#### Looking at top 5, 10, and 20 twitter accounts sentiment on the same graph 
```{r}
# I don't use use top_n here because then it would compared relative size of tweets to it's own data frame (making it look like top_20 had less tweets)

ggplot() +
  
  geom_point(data = TopX_list[["top_5"]], aes(month, sentiment, color = "Top 5"), size = 2) +
  geom_smooth(data = TopX_list[["top_5"]], aes(month, sentiment), se = FALSE, color = "green") +
  
  geom_point(data = TopX_list[["top_10"]], aes(month, sentiment, color = "Top 10"), size = 2) +
  geom_smooth(data = TopX_list[["top_10"]], aes(month, sentiment), se = FALSE, color = "red") +
 
  geom_point(data = TopX_list[["top_20"]], aes(month, sentiment, color = "Top 20"), size = 2) +
  geom_smooth(data = TopX_list[["top_20"]], aes(month, sentiment), se = FALSE, color = "blue") +
  
  scale_color_manual(values = c("Top 5" = "green", "Top 10" = "red",  "Top 20" = "blue")) + 
                     #labels = c("Top 5", "Top 10", "top 20")) +
  
  labs(
    title = "Average Sentiment Score by Tweeters", 
    x = "Year", 
    y = "Sentiment Score (-6 to 6)",
    color = NULL
    )+
  
  scale_y_continuous(
    breaks = seq(-2, 2, by = 1),
    limits = c(-2, 2)) +
  
  #Organizes the x axis by each month then formats how we want it labeled 
   scale_x_date(
    date_breaks = "1 year",
    date_labels = "%b '%y") +
    #date_minor_breaks = "1 month", minor_breaks = "1 month") +
  
  theme_clean() +
  
  theme(panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
    legend.position = c(.5, .2),
    legend.direction = "horizontal",
    legend.text = element_text(size = 10)) 
```


### Sentiment Analysis of the specific top 20 twitter accounts posting about China's BRI (looking at the individual users)
```{r}
top_users_list <- list()

#Creating 20 different dataframes based on the top 20 tweeters  

for (user in top_20_a) {
  hum <- afinn_scores %>% filter(username == user) %>%
    group_by(month) %>% 
    summarise(sentiment = mean(value), n_tweets = n()) %>%
    mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))
  
  top_users_list[[user]] <- hum 
  
  rm(hum)
}

#list of top 20 users to choose from 
names(top_users_list)

top_users_list[["orfonline"]]
```


```{r}
#Code to iterate through the top 20 tweeters to create the graphs for each user

top20_user_plots <- list()

# Loop through time data frames and create plots for the factor varaible teeth 
for (user in names(top_users_list)) { #Equivalent of "range(len(dfs_times))" in python 
  
  plot <- ggplot(top_users_list[[user]], aes(month, sentiment)) +
    geom_line(color = "red", size = 1) +
    geom_point(aes(size = n_tweets), alpha = 0.6, color = "red") +
    
    labs(
    title = paste0("Username: " , user) , 
    x = "Month", 
    y = "Sentiment Score (-6 to 6)") +
    
    #Organizes the x axis by each month then formats how we want it labeled 
    scale_x_date(
      date_breaks = "1 year",
      date_labels = "%b '%y") +
    
    theme_clean()
    
  
    top20_user_plots[[user]] <- plot
}

names(top20_user_plots)

top20_user_plots[["hktdc"]]

rm(plot)

#plot_grid(plotlist = top20_user_plots, ncol = 6)
```