sentiments_CSM %>% ggplot(aes(month, sentiment)) +
geom_line(color = "blue", size = 1) +
geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue") +
labs(
title = "Average Sentiment Score by All Offical Chinese State Media",
x = "Year",
y = "Sentiment Score (-6 to 6)",
size = "Tweet Size"
)+
scale_y_continuous(
breaks = seq(-2, 2, by = 1),
limits = c(-2, 2)) +
#Organizes the x axis by each month then formats how we want it labeled
scale_x_date(
date_breaks = "1 year",
date_labels = "%b '%y") +
theme_clean() +
theme(panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
legend.position = c(.5, .3),
legend.direction = "horizontal",
legend.title = element_text(size = 10.5),
legend.text = element_text(size = 10))
#ggsave(one, file = "Cool Figs/CSM_All.png")
outlets <- list()
for (outlet in china_offical_news) {
CSM_outlet <- afinn_scores_b %>%
filter(username == outlet) %>%
group_by(month) %>%
summarise(sentiment = mean(value), n_tweets = n()) %>%
mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))
outlets[[outlet]] <- CSM_outlet
}
china_offical_news
outlets[["CCTV"]]
#graph is scrunched on y axis to focus on smoothing line
ggplot() +
geom_point(data = outlets[["beltroadnews"]], aes(month, sentiment, color = "beltroadnews"), size = 3) +
geom_smooth(data = outlets[["beltroadnews"]], aes(month, sentiment), se = FALSE, color = "red")+
geom_point(data = outlets[["XHNews"]], aes(month, sentiment, color = "XHNews"), size = 3) +
geom_smooth(data = outlets[["XHNews"]], aes(month, sentiment), se = FALSE, color = "blue") +
geom_point(data = outlets[["globaltimesnews"]], aes(month, sentiment, color = "globaltimesnews"), size = 3) +
geom_smooth(data = outlets[["globaltimesnews"]], aes(month, sentiment), se = FALSE, color = "green") +
geom_point(data = outlets[["ChinaDaily"]], aes(month, sentiment, color = "ChinaDaily"), size = 3) +
geom_smooth(data = outlets[["ChinaDaily"]], aes(month, sentiment), se = FALSE, color = "orange") +
scale_color_manual(values = c("beltroadnews" = "red", "XHNews" = "blue",  "globaltimesnews" = "green", "ChinaDaily" = "orange")) +
labs(
title = "Average Sentiment Score of Select CSM",
x = "Year",
y = "Sentiment Score (-6 to 6)",
color = "News Agency") +
scale_y_continuous(
breaks = seq(0, 2, by = .2),
limits = c(0, 2)) +
#Organizes the x axis by each month then formats how we want it labeled
scale_x_date(
date_breaks = "1 year",
date_labels = "%b '%y") +
theme_clean()
#ggsave(one, file = "Cool Figs/SelectCSM.png", width = 12, height =7)
#Graph plotting Top 5, top 10, all tweets, and Chinese state media tweets together on the same plot to see their influence
ggplot() +
geom_point(data = tweet_sentiments_a, aes(month, sentiment, color = "All Tweets"), size = 3) +
geom_smooth(data = tweet_sentiments_a, aes(month, sentiment), se = FALSE, color = "red")+
geom_point(data = TopX_list[["top_5"]], aes(month, sentiment, color = "Top 5"), size = 3) +
geom_smooth(data = TopX_list[["top_5"]], aes(month, sentiment), se = FALSE, color = "blue") +
#  geom_point(data = TopX_list[["top_10"]], aes(month, sentiment, color = "Top 10"), size = 3) +
#  geom_smooth(data = TopX_list[["top_10"]], aes(month, sentiment), se = FALSE, color = "purple") +
#  geom_point(data = TopX_list[["top_20"]], aes(month, sentiment, color = "Top 20"), size = 3) +
#  geom_smooth(data = TopX_list[["top_20"]], aes(month, sentiment), se = FALSE, color = "green") +
geom_point(data = sentiments_CSM, aes(month, sentiment, color = "Chinese State Media"), size = 3) +
geom_smooth(data = sentiments_CSM, aes(month, sentiment), se = FALSE, color = "orange") +
scale_color_manual(values = c("All Tweets" = "red", "Top 5" = "blue", "Top 10" = "purple", "Top 20" = "green", "Chinese State Media" = "orange")) +
#labels = c("All Tweeters", "Top Ten", "Top Five", "CSM")) +
labs(
title = "Average Sentiment Score from 2014-2022",
x = "Year",
y = "Sentiment Score (-6 to 6)",
color = NULL) +
#scale_y_continuous(
#breaks = seq(-0.5, 2, by = .2),
#limits = c(-0.5, 2)) +
#Organizes the x axis by each month then formats how we want it labeled
scale_x_date(
date_breaks = "1 year",
date_labels = "%b '%y") +
theme_clean() +
theme(panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
legend.position = c(.7, .9),
legend.direction = "horizontal",
legend.title = element_text(size = 10.5),
legend.text = element_text(size = 10))
#ggsave(one, file = "Cool Figs/Top5CSMAll.png")
ggplot() +
geom_point(data = tweet_sentiments_a, aes(month, sentiment, color = "All Tweets"), size = 3) +
geom_smooth(data = tweet_sentiments_a, aes(month, sentiment), color = "red")+
#  geom_point(data = TopX_list[["top_5"]], aes(month, sentiment, color = "Top 5"), size = 3) +
#  geom_smooth(data = TopX_list[["top_5"]], aes(month, sentiment), se = FALSE, color = "blue") +
#  geom_point(data = TopX_list[["top_10"]], aes(month, sentiment, color = "Top 10"), size = 3) +
#  geom_smooth(data = TopX_list[["top_10"]], aes(month, sentiment), se = FALSE, color = "purple") +
#  geom_point(data = TopX_list[["top_20"]], aes(month, sentiment, color = "Top 20"), size = 3) +
#  geom_smooth(data = TopX_list[["top_20"]], aes(month, sentiment), se = FALSE, color = "green") +
geom_point(data = sentiments_CSM, aes(month, sentiment, color = "Chinese State Media"), size = 3) +
geom_smooth(data = sentiments_CSM, aes(month, sentiment), color = "orange") +
scale_color_manual(values = c("All Tweets" = "red", "Top 5" = "blue", "Top 10" = "purple", "Top 20" = "green", "Chinese State Media" = "orange")) +
#labels = c("All Tweeters", "Top Ten", "Top Five", "CSM")) +
labs(
title = "Average Sentiment Score from 2014-2023",
x = "Month",
y = "Sentiment Score (-6 to 6)",
color = "Tweet Source") +
scale_y_continuous(
breaks = seq(-2, 2, by = 1),
limits = c(-2, 2)) +
#Organizes the x axis by each month then formats how we want it labeled
scale_x_date(
date_breaks = "1 year",
date_labels = "%b '%y",
expand = c(0.1, 1)) +
theme_clean() +
theme(panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
legend.position = c(.7, .23),
legend.direction = "horizontal",
legend.title = element_text(size = 10.5),
legend.text = element_text(size = 10))
#ggsave(one, file = "Cool Figs/AllTweetsVCSM.png", width = 9, height = 6) #trying out different dimensions
#standardizing the bing lexicon to compare (positive = 1, negative = -1) then averaging them out to plot them over time
bing_scores$stan <- ifelse(bing_scores$sentiment == "positive", 1, -1)
#Creating a dataframe called tweet_sentiments that contains the month, average sentiment of tweets for that month, and number of tweets
#n_tweets = n() is used to create a new column in the summarized data frame that contains the number of tweets per month.
#When using dplyr functions like summarize(), you can use the n() function to count the number of rows in a group. In this case, we're grouping the data by month (in the format month-year) using group_by(month), and then counting the number of rows in each group (i.e., the number of tweets in each month) using n()
tweet_sentiments_b <- bing_scores %>%
group_by(month, year) %>%
summarise(sentiment = mean(stan), n_tweets = n()) %>%
#Changing the format of the month variable back into a date format so that we can use the geom_smooth function with it
mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))
#modification of afinn tweet_sentiments to be based on standardized column for comparison against bing
tweet_sentiments_a_s <- afinn_scores %>%
group_by(month, year) %>%
summarise(sentiment = mean(stan), n_tweets = n()) %>%
#Changing the format of the month variable back into a date format so that we can use the geom_smooth function with it
mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))
tweet_sentiments_b %>% ggplot(aes(month, sentiment)) +
#geom_smooth(color = "red", linetype = "solid", se = FALSE) +
geom_line(color = "blue", size = 1) +
geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue") +
# Add vertical lines for each year
geom_vline(aes(xintercept = as.numeric(as.Date(paste0(year, "-01-01")))),
color = "gray", linetype = "dotted") +
labs(
title = "Average Sentiment of Tweets Using Bing Lexicon",
x = "Year",
y = "Sentiment Score (-1 to 1)",
size = "Tweet Size"
)+
#scale_y_continuous(
#breaks = seq(-1, 1, by = .2),
#limits = c(-1, 1)) +
#Organizes the x axis by each month then formats how we want it labeled
scale_x_date(
date_breaks = "1 year",
date_labels = "%b '%y") +
#date_minor_breaks = "1 month", minor_breaks = "1 month") +
theme_clean() +
theme(panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
legend.position = c(.7, .2),
legend.direction = "horizontal",
legend.title = element_text(size = 10.5),
legend.text = element_text(size = 10))
#ggsave(one, file = "Cool Figs/AvgSentAllBing.png")
sentiment_year_b <- list()
for (yr in unique(tweet_sentiments_b$year)) {
df_year <- subset(tweet_sentiments_b, year == yr)
plot <- ggplot(df_year, aes(month, sentiment)) +
#geom_smooth(color = "red", linetype = "solid", se = FALSE) +
geom_line(color = "blue", size = 1) +
geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue") +
# Add vertical lines for each month
geom_vline(aes(xintercept = month), color = "gray", linetype = "dotted") +
labs(
title = paste0("Average Sentiment of Tweets (Bing) in ", yr),
x = paste0("Month (", yr, ")"),
y = "Sentiment Score (-6 to 6)",
size = "Tweet Size"
)+
#scale_y_continuous(
#breaks = seq(-1, 1, by = .2),
#limits = c(-1, 1)) +
#Organizes the x axis by each month then formats how we want it labeled
scale_x_date(
date_breaks = "1 month",
date_labels = "%b") +
#date_minor_breaks = "1 month", minor_breaks = "1 month") +
theme_clean() +
theme(panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
#legend.position = c(.7, .3),
#legend.direction = "horizontal",
legend.title = element_text(size = 10.5),
legend.text = element_text(size = 10))
sentiment_year_b[[yr]] <- plot
rm(plot, df_year)
}
sentiment_year_b[["2018"]]
#plot_grid(plotlist = sentiment_year, ncol = 3)
ggplot() +
geom_line(data = tweet_sentiments_a_s, aes(month, sentiment, color = "All Tweets (Afinn)"), size = 1) +
geom_line(data = tweet_sentiments_b, aes(month, sentiment, color = "All Tweets (Bing)"), size = 1) +
scale_color_manual(values = c("All Tweets (Afinn)" = "red", "All Tweets (Bing)" = "blue")) +
labs(
title = "Average Sentiment Score of Tweets from 2014 - 2023",
subtitle = "Sentiment standardized on -1 to 1 scale",
x = "Year",
y = "Sentiment Score (-1 to 1)",
size = "Tweet Size",
color = "Lexicon")+
#scale_y_continuous(
#breaks = seq(-2, 2, by = .2),
#limits = c(-2, 2)) +
#Organizes the x axis by each month then formats how we want it labeled
scale_x_date(
date_breaks = "1 year",
date_labels = "%b '%y") +
#date_minor_breaks = "1 month", minor_breaks = "1 month") +
theme_clean() +
theme(panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
legend.position = c(.7, .9),
legend.direction = "horizontal",
legend.title = element_text(size = 10.5),
legend.text = element_text(size = 10))
#ggsave(one, file = "Cool Figs/AfinnVBingAll.png", width = 16, height = 9) #trying out different dimensions 16 by 9
# Present results
#graph is scrunched on y axis to focus on smoothing line
ggplot() +
geom_point(data = outlets[["beltroadnews"]], aes(month, sentiment, color = "beltroadnews"), size = 2) +
geom_smooth(data = outlets[["beltroadnews"]], aes(month, sentiment), se = FALSE, color = "red")+
geom_point(data = outlets[["XHNews"]], aes(month, sentiment, color = "XHNews"), size = 2) +
geom_smooth(data = outlets[["XHNews"]], aes(month, sentiment), se = FALSE, color = "blue") +
geom_point(data = outlets[["globaltimesnews"]], aes(month, sentiment, color = "globaltimesnews"), size = 2) +
geom_smooth(data = outlets[["globaltimesnews"]], aes(month, sentiment), se = FALSE, color = "green") +
geom_point(data = outlets[["ChinaDaily"]], aes(month, sentiment, color = "ChinaDaily"), size = 3) +
geom_smooth(data = outlets[["ChinaDaily"]], aes(month, sentiment), se = FALSE, color = "orange") +
scale_color_manual(values = c("beltroadnews" = "red", "XHNews" = "blue",  "globaltimesnews" = "green", "ChinaDaily" = "orange")) +
labs(
title = "Average Sentiment Score of Select CSM",
x = "Year",
y = "Sentiment Score (-6 to 6)",
color = "News Agency") +
scale_y_continuous(
breaks = seq(0, 2, by = .2),
limits = c(0, 2)) +
#Organizes the x axis by each month then formats how we want it labeled
scale_x_date(
date_breaks = "1 year",
date_labels = "%b '%y") +
theme_clean()
#ggsave(one, file = "Cool Figs/SelectCSM.png", width = 12, height =7)
names(top20_user_plots)
top20_user_plots[["beltandroad1"]]
names(top20_user_plots)
top20_user_plots[["Hamza_si0"]]
names(top20_user_plots)
top20_user_plots[["raghavan1314"]]
#Reading in all the libraries and wrangled data frames from this initial R file
source("C:/Users/Ryan/Coding Projects/Twitter Data Scraping/sent_wrangling.R")
#Gathering a list of all the excel files from the current directory (assuming that only excel files with tweet data are in this file )
tweet_file_list1422 <- list.files(path = "C:/Users/Ryan/Coding Projects/Twitter Data Scraping/data", pattern = "xlsx", full.names = TRUE) #full.names returns relative path instead of just the file name which would be the default option
library(tidyverse)
library(readxl)
library(tidytext)
library(ggthemes)
library(udpipe) #library for lemmatization
library(cowplot)
#Gathering a list of all the excel files from the current directory (assuming that only excel files with tweet data are in this file )
tweet_file_list1422 <- list.files(path = "C:/Users/Ryan/Coding Projects/Twitter Data Scraping/data", pattern = "xlsx", full.names = TRUE) #full.names returns relative path instead of just the file name which would be the default option
df_list <- lapply(tweet_file_list1422, read_excel)
# Use do.call and rbind to combine all data frames in the list to a single dataframe
combined1422 <- do.call(rbind, df_list)
# Preprocessing and cleaning the combined dataframe for all years
## Making a succienct date variable and filtering tweets to equal only english
rm(df_list)
#changing the format of the ID string to display all numbers
combined1422$id <- format(as.character(combined1422$id), width = 20, scientific = FALSE)
#Changing the date variable to a date format because I don't care about the time of tweet's time of day
combined1422$date <- as.Date(combined1422$date, format = '%b %d, %Y')
#grabbing the month and year (12-18 for december 2018) of the tweet tweeted for later analysis to group by month and year
combined1422$month <- format(combined1422$date, "%m-%y")
#grabbing just the year of the tweet tweeted for later analysis to group by year
combined1422$year <- format(combined1422$date, "%Y")
#Grabbing only the tweets that the python program recognized as english
tweets_EN <- combined1422 %>% filter(language == "en")
## Using some regex to clean up each tweet by getting rid of mentions, links, and pictures
#Note that the unnest_tokens function tries to convert tokens into words and strips characters important to twitter such as # and @. A token in twitter is not the same as in regular English. For this reason, instead of using the default token, words, we define a regex that captures twitter character. The pattern appears complex but all we are defining is a patter that starts with @, # or neither and is followed by any combination of letters or digits:
pattern <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
#Some exploration of the resulting words (not show here) reveals a couple of unwanted characteristics in our tokens. First, some of our tokens are just numbers (years for example). We want to remove these and we can find them using the regex ^\d+$. Second, some of our tokens come from a quote and they start with '. We want to remove the ' when it's at the start of a word, so we will use str_replace(). We add these two lines to the code above to generate our final table:
#We use only our English tweets for creating the tweet_words_china dataframe
tweet_words_china <- tweets_EN %>%
mutate(content = str_replace_all(content, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
unnest_tokens(word, content, token = "regex", pattern = pattern) %>%
filter(!word %in% stop_words$word &
!str_detect(word, "^\\d+$")) %>%
mutate(word = str_replace(word, "^'", ""))
rm(combined1422)
## Loading in lemmatization core if it already exists (saves compute time instead of recreating lemmatization core every time)
file_term <- "lemmatization"
directory <- "Analysis/RDAs" #where we're gonna search for the file
# Loop through all files in the directory to search for an older version of this document (file_name)
# If it exists, load the file
for (filename in list.files(directory)) {
#Searches through all the filenames trying to match our file_term to the filename and once it does we then load that file path
if (grepl(file_term, filename)) {
#Combines the directory and name of file we've matched to create a file path so we can load in the RDA file
file_pathy <- file.path(directory, filename)
#loads the lemitization dataframe we created previously
load(file_pathy)
#exits the loop once we've found the file with "lemmatization" in it
break
}
}
#creates a new character vector based on 2 variables that contain words before and after they've gone through lemmitization
word <- lemmatization_core$sentence #the original vector of characters as they appeared in the tweets
library(tidyverse)
library(readxl)
library(tidytext)
library(ggthemes)
library(udpipe) #library for lemmatization
library(cowplot)
#Gathering a list of all the excel files from the current directory (assuming that only excel files with tweet data are in this file )
tweet_file_list1422 <- list.files(path = "C:/Users/Ryan/Coding Projects/Twitter Data Scraping/data", pattern = "xlsx", full.names = TRUE) #full.names returns relative path instead of just the file name which would be the default option
df_list <- lapply(tweet_file_list1422, read_excel)
# Use do.call and rbind to combine all data frames in the list to a single dataframe
combined1422 <- do.call(rbind, df_list)
# Preprocessing and cleaning the combined dataframe for all years
## Making a succienct date variable and filtering tweets to equal only english
rm(df_list)
#changing the format of the ID string to display all numbers
combined1422$id <- format(as.character(combined1422$id), width = 20, scientific = FALSE)
#Changing the date variable to a date format because I don't care about the time of tweet's time of day
combined1422$date <- as.Date(combined1422$date, format = '%b %d, %Y')
#grabbing the month and year (12-18 for december 2018) of the tweet tweeted for later analysis to group by month and year
combined1422$month <- format(combined1422$date, "%m-%y")
#grabbing just the year of the tweet tweeted for later analysis to group by year
combined1422$year <- format(combined1422$date, "%Y")
#Grabbing only the tweets that the python program recognized as english
tweets_EN <- combined1422 %>% filter(language == "en")
## Using some regex to clean up each tweet by getting rid of mentions, links, and pictures
#Note that the unnest_tokens function tries to convert tokens into words and strips characters important to twitter such as # and @. A token in twitter is not the same as in regular English. For this reason, instead of using the default token, words, we define a regex that captures twitter character. The pattern appears complex but all we are defining is a patter that starts with @, # or neither and is followed by any combination of letters or digits:
pattern <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
#Some exploration of the resulting words (not show here) reveals a couple of unwanted characteristics in our tokens. First, some of our tokens are just numbers (years for example). We want to remove these and we can find them using the regex ^\d+$. Second, some of our tokens come from a quote and they start with '. We want to remove the ' when it's at the start of a word, so we will use str_replace(). We add these two lines to the code above to generate our final table:
#We use only our English tweets for creating the tweet_words_china dataframe
tweet_words_china <- tweets_EN %>%
mutate(content = str_replace_all(content, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
unnest_tokens(word, content, token = "regex", pattern = pattern) %>%
filter(!word %in% stop_words$word &
!str_detect(word, "^\\d+$")) %>%
mutate(word = str_replace(word, "^'", ""))
rm(combined1422)
## Loading in lemmatization core if it already exists (saves compute time instead of recreating lemmatization core every time)
file_term <- "lemmatization"
directory <- "Analysis/RDAs" #where we're gonna search for the file
# Loop through all files in the directory to search for an older version of this document (file_name)
# If it exists, load the file
for (filename in list.files(directory)) {
#Searches through all the filenames trying to match our file_term to the filename and once it does we then load that file path
if (grepl(file_term, filename)) {
#Combines the directory and name of file we've matched to create a file path so we can load in the RDA file
file_pathy <- file.path(directory, filename)
#loads the lemitization dataframe we created previously
load(file_pathy)
#exits the loop once we've found the file with "lemmatization" in it
break
}
}
for (filename in list.files(directory)) {
#Searches through all the filenames trying to match our file_term to the filename and once it does we then load that file path
if (grepl(file_term, filename)) {
#Combines the directory and name of file we've matched to create a file path so we can load in the RDA file
file_pathy <- file.path(directory, filename)
#loads the lemitization dataframe we created previously
load(file_pathy)
#exits the loop once we've found the file with "lemmatization" in it
break
}
}
for (filename in list.files(directory)) {
#Searches through all the filenames trying to match our file_term to the filename and once it does we then load that file path
if (grepl(file_term, filename)) {
#Combines the directory and name of file we've matched to create a file path so we can load in the RDA file
file_pathy <- file.path(directory, filename)
#loads the lemitization dataframe we created previously
load(file_pathy)
#exits the loop once we've found the file with "lemmatization" in it
break
}
}
for (filename in list.files(directory)) {
#Searches through all the filenames trying to match our file_term to the filename and once it does we then load that file path
if (grepl(file_term, filename)) {
#Combines the directory and name of file we've matched to create a file path so we can load in the RDA file
file_pathy <- file.path(directory, filename)
#loads the lemitization dataframe we created previously
load(file_pathy)
#exits the loop once we've found the file with "lemmatization" in it
break
}
}
for (filename in list.files(directory)) {
#Searches through all the filenames trying to match our file_term to the filename and once it does we then load that file path
if (grepl(file_term, filename)) {
#Combines the directory and name of file we've matched to create a file path so we can load in the RDA file
file_pathy <- file.path(directory, filename)
#loads the lemitization dataframe we created previously
load(file_pathy)
#exits the loop once we've found the file with "lemmatization" in it
break
}
}
for (filename in list.files(directory)) {
#Searches through all the filenames trying to match our file_term to the filename and once it does we then load that file path
if (grepl(file_term, filename)) {
#Combines the directory and name of file we've matched to create a file path so we can load in the RDA file
file_pathy <- file.path(directory, filename)
#loads the lemitization dataframe we created previously
load(file_pathy)
#exits the loop once we've found the file with "lemmatization" in it
break
}
}
for (filename in list.files(directory)) {
#Searches through all the filenames trying to match our file_term to the filename and once it does we then load that file path
if (grepl(file_term, filename)) {
#Combines the directory and name of file we've matched to create a file path so we can load in the RDA file
file_pathy <- file.path(directory, filename)
#loads the lemitization dataframe we created previously
load(file_pathy)
#exits the loop once we've found the file with "lemmatization" in it
break
}
}
for (filename in list.files(directory)) {
#Searches through all the filenames trying to match our file_term to the filename and once it does we then load that file path
if (grepl(file_term, filename)) {
#Combines the directory and name of file we've matched to create a file path so we can load in the RDA file
file_pathy <- file.path(directory, filename)
#loads the lemitization dataframe we created previously
load(file_pathy)
#exits the loop once we've found the file with "lemmatization" in it
break
}
}
for (filename in list.files(directory)) {
#Searches through all the filenames trying to match our file_term to the filename and once it does we then load that file path
if (grepl(file_term, filename)) {
#Combines the directory and name of file we've matched to create a file path so we can load in the RDA file
file_pathy <- file.path(directory, filename)
#loads the lemitization dataframe we created previously
load(file_pathy)
#exits the loop once we've found the file with "lemmatization" in it
break
}
}
for (filename in list.files(directory)) {
#Searches through all the filenames trying to match our file_term to the filename and once it does we then load that file path
if (grepl(file_term, filename)) {
#Combines the directory and name of file we've matched to create a file path so we can load in the RDA file
file_pathy <- file.path(directory, filename)
#loads the lemitization dataframe we created previously
load(file_pathy)
#exits the loop once we've found the file with "lemmatization" in it
break
}
}
for (filename in list.files(directory)) {
#Searches through all the filenames trying to match our file_term to the filename and once it does we then load that file path
if (grepl(file_term, filename)) {
#Combines the directory and name of file we've matched to create a file path so we can load in the RDA file
file_pathy <- file.path(directory, filename)
#loads the lemitization dataframe we created previously
load(file_pathy)
#exits the loop once we've found the file with "lemmatization" in it
break
}
}
file_term <- "lemmatization"
directory <- "C:/Users/Ryan/Coding Projects/Twitter Data Scraping/Analysis/RDAs" #where we're gonna search for the file
# Loop through all files in the directory to search for an older version of this document (file_name)
# If it exists, load the file
for (filename in list.files(directory)) {
#Searches through all the filenames trying to match our file_term to the filename and once it does we then load that file path
if (grepl(file_term, filename)) {
#Combines the directory and name of file we've matched to create a file path so we can load in the RDA file
file_pathy <- file.path(directory, filename)
#loads the lemitization dataframe we created previously
load(file_pathy)
#exits the loop once we've found the file with "lemmatization" in it
break
}
}
#Reading in all the libraries and wrangled data frames from this initial R file
source("C:/Users/Ryan/Coding Projects/Twitter Data Scraping/sent_wrangling.R")
#prepping to load in the third lexicon bing where I'll be looking at if a word is either positive or negative
bing <- get_sentiments("bing")
View(bing)
#Reading in all the libraries and wrangled data frames from this initial R file
source("C:/Users/Ryan/Coding Projects/Twitter Data Scraping/sent_wrangling.R")
#In sentiment analysis we assign a word to one or more "sentiment". Although this approach will miss context dependent sentiments, such as sarcasm, when performed on large numbers of words, summaries can provide insights.
#The first step in sentiment analysis is to assign a sentiment to each word. The tidytext package includes several maps or lexicons in the object sentiments:
#For this sentiment analysis I will be using the 'AFINN' lexicon because of it's number range although there are others I could use like "bing", "nrc", or "loughran". Read this help file: ?sentiments
#The AFINN lexicon assigns a score between -5 and 5, with -5 the most negative and 5 the most positive.
afinn <- get_sentiments("afinn")
#exploring afinn dataset
afinn %>% filter(value == 4)
#Code to check what words are in the afinn database
afinn %>% filter(word == 'prosper')
