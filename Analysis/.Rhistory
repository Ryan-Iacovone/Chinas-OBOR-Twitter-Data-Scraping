labs(
title = "Average Sentiment Score by Month",
x = "Year",
y = "Sentiment Score (-6 to 6)",
color = "News Agency") +
scale_y_continuous(
breaks = seq(0, 2, by = .2),
limits = c(0, 2)) +
#Organizes the x axis by each month then formats how we want it labeled
scale_x_date(
date_breaks = "1 year",
date_labels = "%b '%y") +
theme_clean()
#Graph plotting Top 5, top 10, all tweets, and Chinese state media tweets together on the same plot to see their influence
ggplot() +
geom_point(data = tweet_sentiments_a, aes(month, sentiment, color = "All Tweets"), size = 3) +
geom_smooth(data = tweet_sentiments_a, aes(month, sentiment), se = FALSE, color = "red")+
geom_point(data = TopX_list[["top_5"]], aes(month, sentiment, color = "Top 5"), size = 3) +
geom_smooth(data = TopX_list[["top_5"]], aes(month, sentiment), se = FALSE, color = "blue") +
#  geom_point(data = TopX_list[["top_10"]], aes(month, sentiment, color = "Top 10"), size = 3) +
#  geom_smooth(data = TopX_list[["top_10"]], aes(month, sentiment), se = FALSE, color = "purple") +
#  geom_point(data = TopX_list[["top_20"]], aes(month, sentiment, color = "Top 20"), size = 3) +
#  geom_smooth(data = TopX_list[["top_20"]], aes(month, sentiment), se = FALSE, color = "green") +
geom_point(data = sentiments_CSM, aes(month, sentiment, color = "Chinese State Media"), size = 3) +
geom_smooth(data = sentiments_CSM, aes(month, sentiment), se = FALSE, color = "orange") +
scale_color_manual(values = c("All Tweets" = "red", "Top 5" = "blue", "Top 10" = "purple", "Top 20" = "green", "Chinese State Media" = "orange")) +
#labels = c("All Tweeters", "Top Ten", "Top Five", "CSM")) +
labs(
title = "Average Sentiment Score from 2014-2022",
x = "Year",
y = "Sentiment Score (-6 to 6)",
color = "Group") +
#scale_y_continuous(
#breaks = seq(-0.5, 2, by = .2),
#limits = c(-0.5, 2)) +
#Organizes the x axis by each month then formats how we want it labeled
scale_x_date(
date_breaks = "1 year",
date_labels = "%b '%y") +
theme_clean() +
theme(panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"))
ggplot() +
geom_point(data = tweet_sentiments_a, aes(month, sentiment, color = "All Tweets"), size = 3) +
geom_smooth(data = tweet_sentiments_a, aes(month, sentiment), color = "red")+
#  geom_point(data = TopX_list[["top_5"]], aes(month, sentiment, color = "Top 5"), size = 3) +
#  geom_smooth(data = TopX_list[["top_5"]], aes(month, sentiment), se = FALSE, color = "blue") +
#  geom_point(data = TopX_list[["top_10"]], aes(month, sentiment, color = "Top 10"), size = 3) +
#  geom_smooth(data = TopX_list[["top_10"]], aes(month, sentiment), se = FALSE, color = "purple") +
#  geom_point(data = TopX_list[["top_20"]], aes(month, sentiment, color = "Top 20"), size = 3) +
#  geom_smooth(data = TopX_list[["top_20"]], aes(month, sentiment), se = FALSE, color = "green") +
geom_point(data = sentiments_CSM, aes(month, sentiment, color = "Chinese State Media"), size = 3) +
geom_smooth(data = sentiments_CSM, aes(month, sentiment), color = "orange") +
scale_color_manual(values = c("All Tweets" = "red", "Top 5" = "blue", "Top 10" = "purple", "Top 20" = "green", "Chinese State Media" = "orange")) +
#labels = c("All Tweeters", "Top Ten", "Top Five", "CSM")) +
labs(
title = "Average Sentiment Score from 2014-2023",
x = "Month",
y = "Sentiment Score (-6 to 6)",
color = "Tweet Source") +
scale_y_continuous(
breaks = seq(-2, 2, by = 1),
limits = c(-2, 2)) +
#Organizes the x axis by each month then formats how we want it labeled
scale_x_date(
date_breaks = "1 year",
date_labels = "%b '%y") +
theme_clean() +
theme(panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"))
#standardizing the bing lexicon to compare (positive = 1, negative = -1) then averaging them out to plot them over time
bing_scores$stan <- ifelse(bing_scores$sentiment == "positive", 1, -1)
#Creating a dataframe called tweet_sentiments that contains the month, average sentiment of tweets for that month, and number of tweets
#n_tweets = n() is used to create a new column in the summarized data frame that contains the number of tweets per month.
#When using dplyr functions like summarize(), you can use the n() function to count the number of rows in a group. In this case, we're grouping the data by month (in the format month-year) using group_by(month), and then counting the number of rows in each group (i.e., the number of tweets in each month) using n()
tweet_sentiments_b <- bing_scores %>%
group_by(month, year) %>%
summarise(sentiment = mean(stan), n_tweets = n()) %>%
#Changing the format of the month variable back into a date format so that we can use the geom_smooth function with it
mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))
#modification of afinn tweet_sentiments to be based on standardized column for comparison against bing
tweet_sentiments_a_s <- afinn_scores %>%
group_by(month, year) %>%
summarise(sentiment = mean(stan), n_tweets = n()) %>%
#Changing the format of the month variable back into a date format so that we can use the geom_smooth function with it
mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))
tweet_sentiments_b %>% ggplot(aes(month, sentiment)) +
#geom_smooth(color = "red", linetype = "solid", se = FALSE) +
geom_line(color = "blue", size = 1) +
geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue") +
# Add vertical lines for each year
geom_vline(aes(xintercept = as.numeric(as.Date(paste0(year, "-01-01")))),
color = "gray", linetype = "dotted") +
labs(
title = "Average Sentiment of Tweets Using Bing Lexicon",
x = "Year",
y = "Sentiment Score (-1 to 1)",
size = "Tweet Size"
)+
#scale_y_continuous(
#breaks = seq(-1, 1, by = .2),
#limits = c(-1, 1)) +
#Organizes the x axis by each month then formats how we want it labeled
scale_x_date(
date_breaks = "1 year",
date_labels = "%b '%y") +
#date_minor_breaks = "1 month", minor_breaks = "1 month") +
theme_clean() +
theme(panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
legend.position = c(.7, .2),
legend.direction = "horizontal",
legend.title = element_text(size = 10.5),
legend.text = element_text(size = 10))
sentiment_year_b <- list()
for (yr in unique(tweet_sentiments_b$year)) {
df_year <- subset(tweet_sentiments_b, year == yr)
plot <- ggplot(df_year, aes(month, sentiment)) +
#geom_smooth(color = "red", linetype = "solid", se = FALSE) +
geom_line(color = "blue", size = 1) +
geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue") +
# Add vertical lines for each month
geom_vline(aes(xintercept = month), color = "gray", linetype = "dotted") +
labs(
title = paste0("Average Sentiment of Tweets (Bing) in ", yr),
x = paste0("Month (", yr, ")"),
y = "Sentiment Score (-6 to 6)",
size = "Tweet Size"
)+
#scale_y_continuous(
#breaks = seq(-1, 1, by = .2),
#limits = c(-1, 1)) +
#Organizes the x axis by each month then formats how we want it labeled
scale_x_date(
date_breaks = "1 month",
date_labels = "%b") +
#date_minor_breaks = "1 month", minor_breaks = "1 month") +
theme_clean() +
theme(panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
#legend.position = c(.7, .3),
#legend.direction = "horizontal",
legend.title = element_text(size = 10.5),
legend.text = element_text(size = 10))
sentiment_year_b[[yr]] <- plot
rm(plot, df_year)
}
sentiment_year_b[["2018"]]
#plot_grid(plotlist = sentiment_year, ncol = 3)
ggplot() +
geom_line(data = tweet_sentiments_a_s, aes(month, sentiment, color = "All Tweets (Afinn)"), size = 1) +
geom_line(data = tweet_sentiments_b, aes(month, sentiment, color = "All Tweets (Bing)"), size = 1) +
scale_color_manual(values = c("All Tweets (Afinn)" = "red", "All Tweets (Bing)" = "blue")) +
labs(
title = "Average Sentiment Score of Tweets from 2014 - 2023",
subtitle = "Sentiment standardized on -1 to 1 scale",
x = "Year",
y = "Sentiment Score (-1 to 1)",
size = "Tweet Size",
color = "Lexicon"
)+
#scale_y_continuous(
#breaks = seq(-2, 2, by = .2),
#limits = c(-2, 2)) +
#Organizes the x axis by each month then formats how we want it labeled
scale_x_date(
date_breaks = "1 year",
date_labels = "%b '%y") +
#date_minor_breaks = "1 month", minor_breaks = "1 month") +
theme_clean() +
theme(
panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
legend.position = c(.7, .9),
legend.direction = "horizontal",
legend.title = element_text(size = 10.5),
legend.text = element_text(size = 10)
)
# Present results
#Reading in all the libraries and wrangled data frames from this initial R file
source("C:/Users/Ryan/Coding Projects/Twitter Data Scraping/sent_wrangling.R")
#In sentiment analysis we assign a word to one or more "sentiment". Although this approach will miss context dependent sentiments, such as sarcasm, when performed on large numbers of words, summaries can provide insights.
#The first step in sentiment analysis is to assign a sentiment to each word. The tidytext package includes several maps or lexicons in the object sentiments:
#For this sentiment analysis I will be using the 'AFINN' lexicon because of it's number range although there are others I could use like "bing", "nrc", or "loughran". Read this help file: ?sentiments
#The AFINN lexicon assigns a score between -5 and 5, with -5 the most negative and 5 the most positive.
afinn <- get_sentiments("afinn")
#exploring afinn dataset
afinn %>% filter(value == 4)
#Code to check what words are in the afinn database
afinn %>% filter(word == 'prosper')
#We can combine the words and sentiments using inner_join(), which will only keep words associated with a sentiment.
afinn_scores <- tweet_words_china %>% inner_join(afinn, by = "word")
afinn_scores$sentiment <- ifelse(str_detect(afinn_scores$value, "-"), "negative", "positive")
#investigating the top tweeters that had a word match our lexicon and therefore would be more impactful in calculating actual sentiment
#note that we're counting words (that are in the lexicon) per username, not tweets
afinn_scores %>% count(username, sort = TRUE) %>% top_n(10)
#filtering out misleading words from our sentiment analysis
afinn_scores <- afinn_scores %>% filter(!word %in% c("ambitious"))
afinn_scores %>% filter(word == "welcome")
#List of most common words and their associated sentiment value, filtering out CPEC_Official because their tweets were useless
afinn_scores %>%
filter(username != "CPEC_Official") %>%
group_by(word) %>%
summarize(count = n()) %>%
arrange(desc(count)) %>%
left_join(afinn_scores %>% distinct(word, value), by = "word")
afinn_word_count <- afinn_scores %>%
filter(username != "CPEC_Official") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
#List of most common positively sentiment words
afinn_scores %>%
filter(username != "CPEC_Official" & !str_detect(value, "-")) %>%
group_by(word) %>%
summarize(count = n()) %>%
arrange(desc(count)) %>%
left_join(afinn_scores %>% distinct(word, value), by = "word")
#List of most common negatively sentiment words
afinn_scores %>%
filter(username != "CPEC_Official" & str_detect(value, "-")) %>%
group_by(word) %>%
summarize(count = n()) %>%
arrange(desc(count)) %>%
left_join(afinn_scores %>% distinct(word, value), by = "word")
afinn_bar <- afinn_word_count %>% group_by(sentiment) %>% slice_max(n, n = 20) %>% ungroup() %>% mutate(word = reorder(word, n))
afinn_results <- afinn_bar %>% ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(title = "Afinn",
x = "Contribution to sentiment",
y = NULL) +
theme_clean()
#code that gathers the top 5 and 10 users that we tweeted about China's BRI
top_10_afinn <- afinn_scores %>% count(username, sort = TRUE) %>% top_n(10)
top_5_afinn <- afinn_scores %>% count(username, sort = TRUE) %>% top_n(5)
#Code for separate sentiment analysis just on the top tweeters to see how they are influencing the conversation
#taking the top 10 and top 5 tweeters and putting them into a list
top_ten_a <- top_10_afinn$username
top_five_a <- top_5_afinn$username
#Creating a dataframe called tweet_sentiments that contains the month, average sentiment of tweets for that month, and number of tweets
#n_tweets = n() is used to create a new column in the summarized data frame that contains the number of tweets per month.
#When using dplyr functions like summarize(), you can use the n() function to count the number of rows in a group. In this case, we're grouping the data by month (in the format month-year) using group_by(month), and then counting the number of rows in each group (i.e., the number of tweets in each month) using n()
tweet_sentiments <- afinn_scores %>%
group_by(month, year) %>%
summarise(sentiment = mean(value), n_tweets = n())
#Changing the format of the month variable back into a date format so that we can use the geom_smooth function with it
tweet_sentiments <- tweet_sentiments %>%
mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))
#Ensuring that the correct number of tweets are gathered for tweet_sentiments above
#tweets_master %>% filter(month == '07')
#afinn_scores %>% filter(month == '07')
#prepping to load in the second lexicon NRC where I'll be looking at if a word is either positive or negative because I don't care about other specific feelings
nrc <- get_sentiments("nrc") %>% filter(sentiment %in% c("positive", "negative"))
#exploring NRC dataset
nrc %>% filter(sentiment == "negative")
#Code to check what words are in the nrc database
nrc %>% filter(word == 'prosper')
#We can combine the words and sentiments using inner_join(), which will only keep words associated with a sentiment.
nrc_scores <- tweet_words_china %>% inner_join(nrc, by = "word")
#filtering out misleading words from our sentiment analysis
nrc_scores <- nrc_scores %>% filter(!word %in% c("belt", "shanghai", "president", "silk", "government", "foreign", "rail"))
zzzz <- nrc_scores %>% filter(word == "rail")
#investigating the top tweeters that had a word match our lexicon and therefore would be more impactful in calculating actual sentiment
#note that we're counting words (that are in the lexicon) per username, not tweets
nrc_scores %>% count(username, sort = TRUE) %>% top_n(10)
#List of most common words and their associated sentiment value, filtering out CPEC_Official because their tweets were useless
#This new code version is sooo much cleaner
nrc_word_counts <- nrc_scores %>%
filter(username != "CPEC_Official") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
#List of most common positively sentiment words
nrc_scores %>%
filter(username != "CPEC_Official" & sentiment == "positive") %>%
group_by(word) %>%
summarize(count = n()) %>%
arrange(desc(count)) %>%
left_join(nrc_scores %>% distinct(word, sentiment), by = "word")
#List of most common negatively sentiment words
nrc_scores %>%
filter(username != "CPEC_Official" & sentiment == "negative") %>%
group_by(word) %>%
summarize(count = n()) %>%
arrange(desc(count)) %>%
left_join(nrc_scores %>% distinct(word, sentiment), by = "word")
nrc_bar <- nrc_word_counts %>% group_by(sentiment) %>% slice_max(n, n = 20) %>% ungroup() %>% mutate(word = reorder(word, n))
nrc_results <- nrc_bar %>% ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(title = "NRC",
x = "Contribution to sentiment",
y = NULL) +
theme_clean()
#prepping to load in the third lexicon bing where I'll be looking at if a word is either positive or negative
bing <- get_sentiments("bing")
#exploring bing dataset
bing %>% filter(sentiment == "positive")
#Code to check what words are in the bing database
bing %>% filter(word == 'opportunity')
#We can combine the words and sentiments using inner_join(), which will only keep words associated with a sentiment.
bing_scores <- tweet_words_china %>% inner_join(bing, by = "word")
bing_scores <- bing_scores %>% filter(!word %in% c("trump", "premier", "gold", "vice", "dawn", "soft", "ambitious", "rail"))
zzzz <- bing_scores %>% filter(word  == "ready")
#investigating the top tweeters that had a word match our lexicon and therefore would be more impactful in calculating actual sentiment
#note that we're counting words (that are in the lexicon) per username, not tweets
bing_scores %>% count(username, sort = TRUE) %>% top_n(10)
#List of most common words and their associated sentiment value, filtering out CPEC_Official because their tweets were useless
#This new code version is sooo much cleaner
bing_word_counts <- bing_scores %>%
filter(username != "CPEC_Official") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
#List of most common positively sentiment words
bing_scores %>%
filter(username != "CPEC_Official" & sentiment == "positive") %>%
group_by(word) %>%
summarize(count = n()) %>%
arrange(desc(count)) %>%
left_join(bing_scores %>% distinct(word, sentiment), by = "word")
#List of most common negatively sentiment words
bing_scores %>%
filter(username != "CPEC_Official" & sentiment == "negative") %>%
group_by(word) %>%
summarize(count = n()) %>%
arrange(desc(count)) %>%
left_join(bing_scores %>% distinct(word, sentiment), by = "word")
bing_bar <- bing_word_counts %>% group_by(sentiment) %>% slice_max(n, n = 20) %>% ungroup() %>% mutate(word = reorder(word, n))
bing_results <- bing_bar %>% ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(title = "Bing",
x = "Contribution to sentiment",
y = NULL) +
theme_clean()
plot_grid(afinn_results, nrc_results, bing_results, ncol = 3)
afinn_scores$stan <- ifelse(afinn_scores$sentiment == "positive", 1, -1)
nrc_scores$stan <- ifelse(nrc_scores$sentiment == "positive", 1, -1)
bing_scores$stan <- ifelse(bing_scores$sentiment == "positive", 1, -1)
#Creating a dataframe called tweet_sentiments that contains the month, average sentiment of tweets for that month, and number of tweets
#n_tweets = n() is used to create a new column in the summarized data frame that contains the number of tweets per month.
#When using dplyr functions like summarize(), you can use the n() function to count the number of rows in a group. In this case, we're grouping the data by month (in the format month-year) using group_by(month), and then counting the number of rows in each group (i.e., the number of tweets in each month) using n()
df_list <- list(afinn_scores, nrc_scores, bing_scores)
names(df_list) <- c("afinn", "nrc", "bing")
tweet_sentiments_list <- list()
for (df in names(df_list)) {
tweet_sentiments <- df_list[[df]] %>%
group_by(month, year) %>%
summarise(sent = mean(stan), n_tweets = n()) %>%
mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))
tweet_sentiments_list[[df]] <- tweet_sentiments
rm(tweet_sentiments)
}
tweet_sentiments_list[["nrc"]]
#Ensuring that the correct number of tweets are gathered for tweet_sentiments above
#tweets_master %>% filter(month == '07')
#afinn_scores %>% filter(month == '07')
#Create visualizations
#Code to look at all the years I've collected data for as a whole on a single graph
lex_plots <- list()
for (lex in names(tweet_sentiments_list)) {
plot <- ggplot(tweet_sentiments_list[[lex]],aes(month, sent)) +
#geom_smooth(color = "red", linetype = "solid", se = FALSE) +
geom_line(color = "blue", size = 1) +
geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue", show.legend = FALSE) +
labs(
title = "Average Sentiment Score of Tweets by Month from 2014 - 2021",
subtitle = lex ,
x = "Month",
y = "Sentiment Score (-6 to 6)",
size = "Tweet Size"
)+
#scale_y_continuous(
#breaks = seq(-1, 1, by = .2),
#limits = c(-1, 1)) +
#Organizes the x axis by each month then formats how we want it labeled
scale_x_date(
date_breaks = "1 year",
date_labels = "%b '%y") +
#date_minor_breaks = "1 month", minor_breaks = "1 month") +
theme_clean() +
theme(
panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
legend.position = c(.7, .2),
legend.direction = "horizontal",
legend.title = element_text(size = 10.5),
legend.text = element_text(size = 10)
)
lex_plots[[lex]] <- plot
}
lex_plots[["afinn"]]
plot_grid(plotlist = lex_plots, ncol = 3)
#Reading in all the libraries and wrangled data frames from this initial R file
source("C:/Users/Ryan/Coding Projects/Twitter Data Scraping/sent_wrangling.R")
#prepping to load in the third lexicon bing where I'll be looking at if a word is either positive or negative
bing <- get_sentiments("bing")
#exploring bing dataset
bing %>% filter(sentiment == "positive")
#Code to check what words are in the bing database
bing %>% filter(word == 'opportunity')
#We can combine the words and sentiments using inner_join(), which will only keep words associated with a sentiment.
# bing_scores_b means this data frame is biased because chinese state media sources are included
bing_scores_b <- tweet_words_china %>% inner_join(bing, by = "word")
#tidying up
rm(bing)
#filtering out misleading words from our bing dataset
bing_scores_b <- bing_scores_b %>% filter(!word %in% c("trump", "premier", "gold", "vice", "dawn", "soft", "ambitious", "rail"))
#zzzz <- bing_scores %>% filter(word  == "ready")
#Getting rid of biased Chinese state media sources from overall bing_scores data frame
#official Chinese news sites were gathered by looking up common chinese state media and see if they showed up in our dataset. This was then confirmed by checking each account in question and seeing if it had the "chinese state media" banner on their twitter profiles
china_offical_news <- c("CCTV",  "ChinaDaily", "beltroadnews", "XHNews", "China__Focus", "ChinaEUMission", "CGTNOfficial", "globaltimesnews", "PDChina", "ChinaDailyWorld", "chinafrica1")
bing_scores <- bing_scores_b %>% filter(!username %in% china_offical_news)
#note that we're counting words (that are in the lexicon) per username, not tweets
bing_scores %>% count(username, sort = TRUE) %>% top_n(10)
#This new code version is sooo much cleaner
bing_scores %>%
filter(username != "CPEC_Official") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
bing_word_counts <- bing_scores %>%
filter(username != "CPEC_Official") %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
# Positive sentiment words
bing_scores %>%
filter(username != "CPEC_Official" & sentiment == "positive") %>%
group_by(word) %>%
summarize(count = n()) %>%
arrange(desc(count)) %>%
left_join(bing_scores %>% distinct(word, sentiment), by = "word")
# negative sentiment words
bing_scores %>%
filter(username != "CPEC_Official" & sentiment == "negative") %>%
group_by(word) %>%
summarize(count = n()) %>%
arrange(desc(count)) %>%
left_join(bing_scores %>% distinct(word, sentiment), by = "word")
bing_bar <- bing_word_counts %>% group_by(sentiment) %>% slice_max(n, n = 20) %>% ungroup() %>% mutate(word = reorder(word, n))
#just trying to clean up the global environment
rm(bing_word_counts)
bing_bar %>% ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(title = "Bing",
x = "Contribution to sentiment",
y = NULL) +
theme_clean()
#standardizing the bing lexicon to compare (positive = 1, negative = -1) then averaging them out to plot them over time
bing_scores$stan <- ifelse(bing_scores$sentiment == "positive", 1, -1)
#Creating a dataframe called tweet_sentiments that contains the month, average sentiment of tweets for that month, and number of tweets
#n_tweets = n() is used to create a new column in the summarized data frame that contains the number of tweets per month.
#When using dplyr functions like summarize(), you can use the n() function to count the number of rows in a group. In this case, we're grouping the data by month (in the format month-year) using group_by(month), and then counting the number of rows in each group (i.e., the number of tweets in each month) using n()
tweet_sentiments_b <- bing_scores %>%
group_by(month, year) %>%
summarise(sentiment = mean(stan), n_tweets = n()) %>%
#Changing the format of the month variable back into a date format so that we can use the geom_smooth function with it
mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))
tweet_sentiments_b %>% ggplot(aes(month, sentiment)) +
#geom_smooth(color = "red", linetype = "solid", se = FALSE) +
geom_line(color = "blue", size = 1) +
geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue") +
# Add vertical lines for each year
geom_vline(aes(xintercept = as.numeric(as.Date(paste0(year, "-01-01")))),
color = "gray", linetype = "dotted") +
labs(
title = "Average Sentiment of Tweets Using Bing Lexicon",
x = "Year",
y = "Sentiment Score (-1 to 1)",
size = "Tweet Size"
)+
#scale_y_continuous(
#breaks = seq(-1, 1, by = .2),
#limits = c(-1, 1)) +
#Organizes the x axis by each month then formats how we want it labeled
scale_x_date(
date_breaks = "1 year",
date_labels = "%b '%y") +
#date_minor_breaks = "1 month", minor_breaks = "1 month") +
theme_clean() +
theme(panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
legend.position = c(.7, .2),
legend.direction = "horizontal",
legend.title = element_text(size = 10.5),
legend.text = element_text(size = 10))
sentiment_year_b <- list()
for (yr in unique(tweet_sentiments_b$year)) {
df_year <- subset(tweet_sentiments_b, year == yr)
plot <- ggplot(df_year, aes(month, sentiment)) +
#geom_smooth(color = "red", linetype = "solid", se = FALSE) +
geom_line(color = "blue", size = 1) +
geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue") +
# Add vertical lines for each month
geom_vline(aes(xintercept = month), color = "gray", linetype = "dotted") +
labs(
title = paste0("Average Sentiment of Tweets (Bing) in ", yr),
x = paste0("Month (", yr, ")"),
y = "Sentiment Score (-6 to 6)",
size = "Tweet Size"
)+
#scale_y_continuous(
#breaks = seq(-1, 1, by = .2),
#limits = c(-1, 1)) +
#Organizes the x axis by each month then formats how we want it labeled
scale_x_date(
date_breaks = "1 month",
date_labels = "%b") +
#date_minor_breaks = "1 month", minor_breaks = "1 month") +
theme_clean() +
theme(panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
#legend.position = c(.7, .3),
#legend.direction = "horizontal",
legend.title = element_text(size = 10.5),
legend.text = element_text(size = 10))
sentiment_year_b[[yr]] <- plot
rm(plot, df_year)
}
sentiment_year_b[["2018"]]
#plot_grid(plotlist = sentiment_year, ncol = 3)
