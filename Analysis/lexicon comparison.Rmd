```{r}
#Reading in all the libraries and wrangled data frames from this initial R file 
source("C:/Users/Ryan/Coding Projects/Twitter Data Scraping/sent_wrangling.R")
```

###### Perform sentiment analysis ###### 

#Afinn lexicon 
```{r}
#In sentiment analysis we assign a word to one or more "sentiment". Although this approach will miss context dependent sentiments, such as sarcasm, when performed on large numbers of words, summaries can provide insights.

#The first step in sentiment analysis is to assign a sentiment to each word. The tidytext package includes several maps or lexicons in the object sentiments:

#For this sentiment analysis I will be using the 'AFINN' lexicon because of it's number range although there are others I could use like "bing", "nrc", or "loughran". Read this help file: ?sentiments

#The AFINN lexicon assigns a score between -5 and 5, with -5 the most negative and 5 the most positive.
afinn <- get_sentiments("afinn")

#exploring afinn dataset
afinn %>% filter(value == 4)

#Code to check what words are in the afinn database
afinn %>% filter(word == 'prosper')
```


```{r}
#We can combine the words and sentiments using inner_join(), which will only keep words associated with a sentiment.
afinn_scores <- tweet_words_china %>% inner_join(afinn, by = "word")
```

#adding a sentiment column in the afinn_scores dataset to just look at positive and negative words based on the numbering system
```{r}
afinn_scores$sentiment <- ifelse(str_detect(afinn_scores$value, "-"), "negative", "positive")
```


```{r}
#investigating the top tweeters that had a word match our lexicon and therefore would be more impactful in calculating actual sentiment 
#note that we're counting words (that are in the lexicon) per username, not tweets 

afinn_scores %>% count(username, sort = TRUE) %>% top_n(10)
```

```{r}
#filtering out misleading words from our sentiment analysis
afinn_scores <- afinn_scores %>% filter(!word %in% c("ambitious"))

afinn_scores %>% filter(word == "welcome")
```


#Most common word analyis and their assocaited sentiment
```{r}
#List of most common words and their associated sentiment value, filtering out CPEC_Official because their tweets were useless 

afinn_scores %>%
  filter(username != "CPEC_Official") %>%
  group_by(word) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  left_join(afinn_scores %>% distinct(word, value), by = "word")


afinn_word_count <- afinn_scores %>%
  filter(username != "CPEC_Official") %>% 
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
```


```{r}
#List of most common positively sentiment words

afinn_scores %>%
  filter(username != "CPEC_Official" & !str_detect(value, "-")) %>%
  group_by(word) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  left_join(afinn_scores %>% distinct(word, value), by = "word")

```


```{r}
#List of most common negatively sentiment words 

afinn_scores %>%
  filter(username != "CPEC_Official" & str_detect(value, "-")) %>%
  group_by(word) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  left_join(afinn_scores %>% distinct(word, value), by = "word")

```


#Concise bar chart that nicely plots top negative and positve words from afinn lexicon
```{r}
afinn_bar <- afinn_word_count %>% group_by(sentiment) %>% slice_max(n, n = 20) %>% ungroup() %>% mutate(word = reorder(word, n))

afinn_results <- afinn_bar %>% ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(title = "Afinn",
    x = "Contribution to sentiment",
       y = NULL) +
  theme_clean()
```


#Investigating twitter accounts that who's tweets contain sentiments that match our lexicon list and therefore are more influential in the data 
```{r}
#code that gathers the top 5 and 10 users that we tweeted about China's BRI
top_10_afinn <- afinn_scores %>% count(username, sort = TRUE) %>% top_n(10)
top_5_afinn <- afinn_scores %>% count(username, sort = TRUE) %>% top_n(5)

#Code for separate sentiment analysis just on the top tweeters to see how they are influencing the conversation

#taking the top 10 and top 5 tweeters and putting them into a list
top_ten_a <- top_10_afinn$username
top_five_a <- top_5_afinn$username
```


#Prepping our the data visualization graph 
```{r}
#Creating a dataframe called tweet_sentiments that contains the month, average sentiment of tweets for that month, and number of tweets

#n_tweets = n() is used to create a new column in the summarized data frame that contains the number of tweets per month.
#When using dplyr functions like summarize(), you can use the n() function to count the number of rows in a group. In this case, we're grouping the data by month (in the format month-year) using group_by(month), and then counting the number of rows in each group (i.e., the number of tweets in each month) using n()

tweet_sentiments <- afinn_scores %>% 
  group_by(month, year) %>% 
  summarise(sentiment = mean(value), n_tweets = n())


#Changing the format of the month variable back into a date format so that we can use the geom_smooth function with it
tweet_sentiments <- tweet_sentiments %>%
  mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))


#Ensuring that the correct number of tweets are gathered for tweet_sentiments above 
#tweets_master %>% filter(month == '07') 

#afinn_scores %>% filter(month == '07') 
```




#NRC lexicon 
```{r}
#prepping to load in the second lexicon NRC where I'll be looking at if a word is either positive or negative because I don't care about other specific feelings
nrc <- get_sentiments("nrc") %>% filter(sentiment %in% c("positive", "negative"))
```


```{r}
#exploring NRC dataset
nrc %>% filter(sentiment == "negative")

#Code to check what words are in the nrc database
nrc %>% filter(word == 'prosper')
```


```{r}
#We can combine the words and sentiments using inner_join(), which will only keep words associated with a sentiment.
nrc_scores <- tweet_words_china %>% inner_join(nrc, by = "word")
```


```{r}
#filtering out misleading words from our sentiment analysis
nrc_scores <- nrc_scores %>% filter(!word %in% c("belt", "shanghai", "president", "silk", "government", "foreign", "rail"))

zzzz <- nrc_scores %>% filter(word == "rail")
```

```{r}
#investigating the top tweeters that had a word match our lexicon and therefore would be more impactful in calculating actual sentiment 
#note that we're counting words (that are in the lexicon) per username, not tweets 

nrc_scores %>% count(username, sort = TRUE) %>% top_n(10)
```

#Most common word analyis and their associated sentiment
```{r}
#List of most common words and their associated sentiment value, filtering out CPEC_Official because their tweets were useless 
#This new code version is sooo much cleaner

nrc_word_counts <- nrc_scores %>%
  filter(username != "CPEC_Official") %>% 
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
```


```{r}
#List of most common positively sentiment words

nrc_scores %>%
    filter(username != "CPEC_Official" & sentiment == "positive") %>%
    group_by(word) %>%
    summarize(count = n()) %>%
    arrange(desc(count)) %>%
    left_join(nrc_scores %>% distinct(word, sentiment), by = "word")
```


```{r}
#List of most common negatively sentiment words 

nrc_scores %>%
    filter(username != "CPEC_Official" & sentiment == "negative") %>%
    group_by(word) %>%
    summarize(count = n()) %>%
    arrange(desc(count)) %>%
    left_join(nrc_scores %>% distinct(word, sentiment), by = "word")
```

#Concise bar chart that nicely plots top negative and positve words from nrc lexicon
```{r}
nrc_bar <- nrc_word_counts %>% group_by(sentiment) %>% slice_max(n, n = 20) %>% ungroup() %>% mutate(word = reorder(word, n))

nrc_results <- nrc_bar %>% ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(title = "NRC",
    x = "Contribution to sentiment",
       y = NULL) +
  theme_clean()
```


#Bing lexicon 
```{r}
#prepping to load in the third lexicon bing where I'll be looking at if a word is either positive or negative
bing <- get_sentiments("bing")
```


```{r}
#exploring bing dataset
bing %>% filter(sentiment == "positive")

#Code to check what words are in the bing database
bing %>% filter(word == 'opportunity')
```


```{r}
#We can combine the words and sentiments using inner_join(), which will only keep words associated with a sentiment.
bing_scores <- tweet_words_china %>% inner_join(bing, by = "word")
```

#filtering out misleading words from our bing dataset
```{r}
bing_scores <- bing_scores %>% filter(!word %in% c("trump", "premier", "gold", "vice", "dawn", "soft", "ambitious", "rail"))

zzzz <- bing_scores %>% filter(word  == "ready")
```


```{r}
#investigating the top tweeters that had a word match our lexicon and therefore would be more impactful in calculating actual sentiment 
#note that we're counting words (that are in the lexicon) per username, not tweets 

bing_scores %>% count(username, sort = TRUE) %>% top_n(10)
```

#Most common word analyis and their associated sentiment
```{r}
#List of most common words and their associated sentiment value, filtering out CPEC_Official because their tweets were useless 
#This new code version is sooo much cleaner

bing_word_counts <- bing_scores %>%
  filter(username != "CPEC_Official") %>% 
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
```


```{r}
#List of most common positively sentiment words

bing_scores %>%
    filter(username != "CPEC_Official" & sentiment == "positive") %>%
    group_by(word) %>%
    summarize(count = n()) %>%
    arrange(desc(count)) %>%
    left_join(bing_scores %>% distinct(word, sentiment), by = "word")
```


```{r}
#List of most common negatively sentiment words 

bing_scores %>%
    filter(username != "CPEC_Official" & sentiment == "negative") %>%
    group_by(word) %>%
    summarize(count = n()) %>%
    arrange(desc(count)) %>%
    left_join(bing_scores %>% distinct(word, sentiment), by = "word")
```

#Concise bar chart that nicely plots top negative and positve words from nrc lexicon
```{r}
bing_bar <- bing_word_counts %>% group_by(sentiment) %>% slice_max(n, n = 20) %>% ungroup() %>% mutate(word = reorder(word, n))

bing_results <- bing_bar %>% ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(title = "Bing",
       x = "Contribution to sentiment",
       y = NULL) +
  theme_clean()
```

#comparing the graphs of the 3 lexicons side by side 
```{r}
plot_grid(afinn_results, nrc_results, bing_results, ncol = 3)
```
#Filtering out unimportant words captured by each lexicon
```{r}

```



#standardizing each lexicon to compare (positive = 1, negative = -1 then avering them out to plot them over time 
```{r}
afinn_scores$stan <- ifelse(afinn_scores$sentiment == "positive", 1, -1)
nrc_scores$stan <- ifelse(nrc_scores$sentiment == "positive", 1, -1)
bing_scores$stan <- ifelse(bing_scores$sentiment == "positive", 1, -1)

```



###### Understiand how the various lexicons affect the graphs


#Prepping our the data visualization graph 
```{r}
#Creating a dataframe called tweet_sentiments that contains the month, average sentiment of tweets for that month, and number of tweets

#n_tweets = n() is used to create a new column in the summarized data frame that contains the number of tweets per month.
#When using dplyr functions like summarize(), you can use the n() function to count the number of rows in a group. In this case, we're grouping the data by month (in the format month-year) using group_by(month), and then counting the number of rows in each group (i.e., the number of tweets in each month) using n()

df_list <- list(afinn_scores, nrc_scores, bing_scores)
names(df_list) <- c("afinn", "nrc", "bing")

tweet_sentiments_list <- list()

for (df in names(df_list)) {
tweet_sentiments <- df_list[[df]] %>% 
  group_by(month, year) %>% 
  summarise(sent = mean(stan), n_tweets = n()) %>%
  mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))

  tweet_sentiments_list[[df]] <- tweet_sentiments
  rm(tweet_sentiments)
}

tweet_sentiments_list[["nrc"]]

#Ensuring that the correct number of tweets are gathered for tweet_sentiments above 
#tweets_master %>% filter(month == '07') 

#afinn_scores %>% filter(month == '07') 
```



##### Code that graphs each standardized sentiment lexicon to a line graph 

```{r}
#Create visualizations

#Code to look at all the years I've collected data for as a whole on a single graph 

lex_plots <- list()

for (lex in names(tweet_sentiments_list)) {
plot <- ggplot(tweet_sentiments_list[[lex]],aes(month, sent)) +
  
  #geom_smooth(color = "red", linetype = "solid", se = FALSE) +
  geom_line(color = "blue", size = 1) +
  geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue", show.legend = FALSE) +
  labs(
    title = "Average Sentiment Score of Tweets by Month from 2014 - 2021", 
    subtitle = lex , 
    x = "Month", 
    y = "Sentiment Score (-6 to 6)",
    size = "Tweet Size"
    )+
  
  #scale_y_continuous(
    #breaks = seq(-1, 1, by = .2),
    #limits = c(-1, 1)) +
  
  #Organizes the x axis by each month then formats how we want it labeled 
   scale_x_date(
    date_breaks = "1 year",
    date_labels = "%b '%y") +
    #date_minor_breaks = "1 month", minor_breaks = "1 month") +
  
  theme_clean() +
  
  theme(
    panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
    
    legend.position = c(.7, .2),
    legend.direction = "horizontal",
    legend.title = element_text(size = 10.5), 
    legend.text = element_text(size = 10) 
    ) 
  
  lex_plots[[lex]] <- plot
}

lex_plots[["afinn"]]

plot_grid(plotlist = lex_plots, ncol = 3)
```