```{r}
library(tidyverse)
library(readxl)
library(tidytext)
library(ggthemes)
library(udpipe) #library for lemmatization 
library(cowplot) 

#Gathering a list of all the excel files from the current directory (assuming that only excel files with tweet data are in this file )
tweet_file_list1422 <- list.files(path = "data", pattern = "xlsx", full.names = TRUE) #full.names returns relative path instead of just the file name which would be the default option 

df_list <- lapply(tweet_file_list1422, read_excel)

# Use do.call and rbind to combine all data frames in the list to a single dataframe
combined1422 <- do.call(rbind, df_list)
```


# Preprocessing and cleaning the combined dataframe for all years  


## Making a succienct date variable and filtering tweets to equal only english 
```{r}
rm(df_list)

#changing the format of the ID string to display all numbers 
combined1422$id <- format(as.character(combined1422$id), width = 20, scientific = FALSE)

#Changing the date variable to a date format because I don't care about the time of tweet's time of day 
combined1422$date <- as.Date(combined1422$date, format = '%b %d, %Y')

#grabbing the month and year (12-18 for december 2018) of the tweet tweeted for later analysis to group by month and year 
combined1422$month <- format(combined1422$date, "%m-%y") 

#grabbing just the year of the tweet tweeted for later analysis to group by year 
combined1422$year <- format(combined1422$date, "%Y") 

#Grabbing only the tweets that the python program recognized as english
tweets_EN <- combined1422 %>% filter(language == "en") 
```


## Using some regex to clean up each tweet by getting rid of mentions, links, and pictures  
```{r}
#Note that the unnest_tokens function tries to convert tokens into words and strips characters important to twitter such as # and @. A token in twitter is not the same as in regular English. For this reason, instead of using the default token, words, we define a regex that captures twitter character. The pattern appears complex but all we are defining is a patter that starts with @, # or neither and is followed by any combination of letters or digits:

pattern <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"

#Some exploration of the resulting words (not show here) reveals a couple of unwanted characteristics in our tokens. First, some of our tokens are just numbers (years for example). We want to remove these and we can find them using the regex ^\d+$. Second, some of our tokens come from a quote and they start with '. We want to remove the ' when it's at the start of a word, so we will use str_replace(). We add these two lines to the code above to generate our final table:
#We use only our English tweets for creating the tweet_words_china dataframe

tweet_words_china <- tweets_EN %>% 
  mutate(content = str_replace_all(content, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
  unnest_tokens(word, content, token = "regex", pattern = pattern) %>%
  filter(!word %in% stop_words$word &
           !str_detect(word, "^\\d+$")) %>%
  mutate(word = str_replace(word, "^'", ""))

rm(combined1422)
```



## Further preprocessing steps by lemmatizing the variable word to it's most basic form  

### ONLY RUN THIS CODE AGAIN IF WE NEED TO CREATE ANOTHER lemmatization_core.RDA FILE TAKES A WHILE TO RUN
```{r eval=FALSE, include=FALSE}
#Creating a temporary copy of our tweet_words dataframe to  add a temporary column 
fake_TWC <- tweet_words_china

#Creating a new word column that eliminates all the # and @ signs if they're in the word variable to then pass to our lemmatization function
fake_TWC$word_nohash <- gsub("[#@]", "", fake_TWC$word)

#Loops through all the words in our variable word then returns the simpliest form of that word among other cool stuff like part of speech
lemmatization_all <- udpipe(fake_TWC$word_nohash, object = 'english')

#Code to get rid of the 's lemmatizations (it would try to turn a word like "father's" into "father" and " 's " but obviously we only care about the word)
lemmatization_core <- lemmatization_all %>% filter(!(xpos %in% c("POS", "PRP")))

#removing the lemmatization_all and fake_TWC data frames since they are not longer useful 
rm(lemmatization_all, fake_TWC)

#Code to write an r data file (rda) 
save(lemmatization_core, file = "C:/Users/Ryan/Coding Projects/Twitter Data Scraping/lemmatization_core.rda")

#Code to then load that RDA file back in 
load("C:/Users/Ryan/Coding Projects/Twitter Data Scraping/lemmatization_core.rda")
```


## Loading in lemmatization core if it already exists (saves compute time instead of recreating lemmatization core every time)
```{r}
file_term <- "lemmatization"
directory <- "C:/Users/Ryan/Coding Projects/Twitter Data Scraping/Analysis/RDAs" #where we're gonna search for the file 

# Loop through all files in the directory to search for an older version of this document (file_name)
# If it exists, load the file
for (filename in list.files(directory)) {
  
  #Searches through all the filenames trying to match our file_term to the filename and once it does we then load that file path  
  if (grepl(file_term, filename)) {
    
    #Combines the directory and name of file we've matched to create a file path so we can load in the RDA file 
    file_pathy <- file.path(directory, filename)
    
    #loads the lemitization dataframe we created previously 
    load(file_pathy)
    
    #exits the loop once we've found the file with "lemmatization" in it 
    break
  }  
}    
```


```{r}
#creates a new character vector based on 2 variables that contain words before and after they've gone through lemmitization   
word <- lemmatization_core$sentence #the original vector of characters as they appeared in the tweets 
lemma <- lemmatization_core$lemma #The lemmitized version of each word now ready for sentiment analysis  

#Create a dictionary as the combined inputs of the two vectors above "word" and "lemma" so their values are now connected via the key of the original word
lemma_dict <- setNames(lemma, word)

#Code to search through the lemma_dict for a specific word to be lemmitized ensuring that it works as intended 
word_to_lookup <- "opportunities"

ifelse(word_to_lookup %in% names(lemma_dict), lemma_dict[[word_to_lookup]], NA)
```


### Finishing up the lemitization process 
```{r}
#Going through the list of our original words then using the match function to connect them with their lemmitized version 

# create a vector of original words to lemmatize, ONLY RUN THIS CODE ONCE OTHERWISE IT CREATES MORE VARIABLES!!!!!!!
orig_words <- tweet_words_china$word

# use the match function to replace the original words with their lemmas
lemmatized_words <- lemma_dict[match(orig_words, names(lemma_dict))]

# replace any missing lemmas with the original words, idk if I want to do this
#lemmatized_words[is.na(lemmatized_words)] <- orig_words[is.na(lemmatized_words)]

# add the lemmatized words to the dataframe, ONLY RUN THIS CODE ONCE OTHERWISE IT CREATES MORE VARIABLES!!!!!!!
tweet_words_china$word_lems <- lemmatized_words

# rename the columns
names(tweet_words_china)[names(tweet_words_china) == "word"] <- "orig_word"
names(tweet_words_china)[names(tweet_words_china) == "word_lems"] <- "word"
```



# Doing a little analysis on the now processed data to ensure it worked


## Gaining insight into the most common words that were tweeted ensuring there's a difference between the original word and then lemmitized version
```{r}
# Most common words that appear in our orig_words list before lemmatization and regex (includes # and @ and all versions of words)
tweet_words_china %>% 
  count(orig_word) %>%
  arrange(desc(n))

#code that achieves the same result as above
#tweet_words_china %>% count(word, sort = TRUE)

#Most common words after lemmitization has occurred using the 'word' list
tweet_words_china %>% 
  count(word) %>%
  top_n(15, n) %>%
  mutate(word = reorder(word, n)) %>%
  arrange(desc(n))
```


## Investigating weird words area 
```{r eval=FALSE, include=FALSE}
bad_word = "@cpec"

grr_lem <- tweet_words_china %>% filter(orig_word == bad_word)

humm_TWC <- tweet_words_china %>% filter(word == bad_word) 


rm(grr_lem, humm_TWC)
```


## Gaining insight into the twitter accounts that tweeted 
###(Does not translate to influence on sentiment though because we count words not tweets, so we would need to see who had the most words recognized by the sentiment analysis - which we do later)
```{r}
#code that gathers the top 20 users that tweeted about China's BRI
tweets_EN %>% count(username, sort = TRUE) %>% top_n(20)
```



# Sentiment Analysis 


## Afinn lexicon

### Prerpocessing Afinn lexicon with our own data 
```{r}
#In sentiment analysis we assign a word to one or more "sentiment". Although this approach will miss context dependent sentiments, such as sarcasm, when performed on large numbers of words, summaries can provide insights.

#The first step in sentiment analysis is to assign a sentiment to each word. The tidytext package includes several maps or lexicons in the object sentiments:

#For this sentiment analysis I will be using the 'AFINN' lexicon because of it's number range although there are others I could use like "bing", "nrc", or "loughran". Read this help file: ?sentiments

#The AFINN lexicon assigns a score between -5 and 5, with -5 the most negative and 5 the most positive.
afinn <- get_sentiments("afinn")

#exploring afinn dataset
afinn %>% filter(value == 4)

#Code to check what words are in the afinn database
afinn %>% filter(word == 'prosper')
```


```{r}
#We can combine the sentiments and words from out china databse using inner_join(), which will only keep words associated with a sentiment.
#"afinn_scores_b" means that the dataset is biased because it still contains chinese affiliated news outlets 

afinn_scores_b <- tweet_words_china %>% inner_join(afinn, by = "word")
```


#### tidying up our affin data frame
```{r}
rm(afinn)

#adding a sentiment column in the afinn_scores dataset to just look at positive and negative words based on the numbering system
afinn_scores_b$sentiment <- ifelse(str_detect(afinn_scores_b$value, "-"), "negative", "positive")

#With the new sentiment column we're standardizing the values for comparison against the bing lexicon 
afinn_scores_b$stan <- ifelse(afinn_scores_b$sentiment == "positive", 1, -1)
```


#### Getting rid of biased Chinese state media sources from overall afinn_scores data frame 
```{r}
#official Chinese news sites were gathered by looking up common chinese state media and see if they showed up in our dataset. This was then confirmed by checking each account in question and seeing if it had the "chinese state media" banner on their twitter profiles

china_offical_news <- c("CCTV",  "ChinaDaily", "beltroadnews", "XHNews", "China__Focus", "ChinaEUMission", "CGTNOfficial", "globaltimesnews", "PDChina", "ChinaDailyWorld", "chinafrica1")

afinn_scores <- afinn_scores_b %>% filter(!username %in% china_offical_news)
```



### Investigating/analysis (no graphs) on Afinn lexicon with our own data 


#### Investigating the top tweeters that had a word match our lexicon and therefore will be more impact in calculating actual sentiment 
```{r}
#note that we're counting words (that are in the lexicon) per username, not tweets 

afinn_scores %>% count(username, sort = TRUE) %>% top_n(20)
```


####List of most common words and their associated sentiment value, filtering out CPEC_Official because their tweets were useless 
```{r}
#Both chunks of code achieve the same results

afinn_scores %>%
  filter(username != "CPEC_Official") %>%
  group_by(word) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  left_join(afinn_scores %>% distinct(word, value), by = "word")

#We save our top words and their associated sentiments into "afinn_word_count" for later analysis with bar charting the top negative and positive words visually
afinn_word_count <- afinn_scores %>%
  filter(username != "CPEC_Official") %>% 
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
```


#### List of most common positive and negative sentiment words
```{r}
# Positive 

afinn_scores %>%
  filter(username != "CPEC_Official" & !str_detect(value, "-")) %>%
  group_by(word) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  left_join(afinn_scores %>% distinct(word, value), by = "word")


# Negative 

afinn_scores %>%
  filter(username != "CPEC_Official" & str_detect(value, "-")) %>%
  group_by(word) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  left_join(afinn_scores %>% distinct(word, value), by = "word")
```


##### Concise bar chart that nicely plots top negative and positive words from afinn lexicon
```{r}
afinn_bar <- afinn_word_count %>% group_by(sentiment) %>% slice_max(n, n = 10) %>% ungroup() %>% mutate(word = reorder(word, n))

#cleaning up the global environment
rm(afinn_word_count)

ggplot(afinn_bar, aes(n, word, fill = sentiment)) +
  
  geom_col(show.legend = FALSE) +
  
  labs(title = "Afinn Lexicon",
     x = "Contribution to sentiment",
     y = NULL) +
  
  facet_wrap(~sentiment, scales = "free_y") +
  
  labs(x = "Contribution to sentiment",
       y = NULL) +
  
  theme_clean() +
  
  theme(plot.title = element_text(hjust = 0.5))

#ggsave(one, file = "Cool Figs/AfinnPosNeg.png")
```



## Bing lexicon


### Prerpocessing and ebasic exploration of the Bing lexicon with our own data (in the same ways we did to the afinn dataset)
```{r}
#prepping to load in the third lexicon bing where I'll be looking at if a word is either positive or negative
bing <- get_sentiments("bing")
```


```{r}
#exploring bing dataset
bing %>% filter(sentiment == "positive")

#Code to check what words are in the bing database
bing %>% filter(word == 'opportunity')
```


```{r}
#We can combine the words and sentiments using inner_join(), which will only keep words associated with a sentiment.
# bing_scores_b means this data frame is biased because chinese state media sources are included 
bing_scores_b <- tweet_words_china %>% inner_join(bing, by = "word")

#tidying up 
rm(bing)
```


```{r}
#filtering out misleading words from our bing dataset
bing_scores_b <- bing_scores_b %>% filter(!word %in% c("trump", "premier", "gold", "vice", "dawn", "soft", "ambitious", "rail"))

#zzzz <- bing_scores %>% filter(word  == "ready")
```


```{r}
#Getting rid of biased Chinese state media sources from overall bing_scores data frame  
bing_scores <- bing_scores_b %>% filter(!username %in% china_offical_news)
```


### Investigating/analysis (no graphs) on Bing lexicon with our own data 

#### Investigating the top tweeters that had a word match our lexicon and therefore would be more impactful in calculating actual sentiment 
```{r}
#note that we're counting words (that are in the lexicon) per username, not tweets 

bing_scores %>% count(username, sort = TRUE) %>% top_n(10)
```


#### List of most common words and their associated sentiment value, filtering out CPEC_Official because their tweets were useless 
```{r}
#This new code version is sooo much cleaner
bing_scores %>%
  filter(username != "CPEC_Official") %>% 
  count(word, sentiment, sort = TRUE) %>%
  ungroup()


bing_word_counts <- bing_scores %>%
  filter(username != "CPEC_Official") %>% 
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
```


#### List of most common postive and negative sentiment words
```{r}
# Positive sentiment words

bing_scores %>%
    filter(username != "CPEC_Official" & sentiment == "positive") %>%
    group_by(word) %>%
    summarize(count = n()) %>%
    arrange(desc(count)) %>%
    left_join(bing_scores %>% distinct(word, sentiment), by = "word")


# negative sentiment words 

bing_scores %>%
    filter(username != "CPEC_Official" & sentiment == "negative") %>%
    group_by(word) %>%
    summarize(count = n()) %>%
    arrange(desc(count)) %>%
    left_join(bing_scores %>% distinct(word, sentiment), by = "word")
```


#### Concise bar chart that nicely plots top negative and positive words from bing lexicon
```{r}
bing_bar <- bing_word_counts %>% group_by(sentiment) %>% slice_max(n, n = 20) %>% ungroup() %>% mutate(word = reorder(word, n))

#just trying to clean up the global environment
rm(bing_word_counts)

bing_bar %>% ggplot(aes(n, word, fill = sentiment)) +
  
  geom_col(show.legend = FALSE) +
  
  facet_wrap(~sentiment, scales = "free_y") +
  
  labs(title = "Bing Lexicon",
       x = "Contribution to sentiment",
       y = NULL) +
  
  theme_clean() +
  
  theme(plot.title = element_text(hjust = 0.5))

#ggsave(one, file = "Cool Figs/BingPosNeg.png")
```




# Visualizations section of sentiment analysis - Code organized by data manipulation then specific graph, rinse and repeat 
        


## Afinn visualizations


###  Prepping afinn_scores for visualization of sentiment over all years in dataset
```{r}
#Creating a dataframe called tweet_sentiments that contains the month, year, average sentiment of tweets for timer period, and number of tweets

#n_tweets = n() is used to create a new column in the summarized data frame that contains the number of tweets per month.
#When using dplyr functions like summarize(), you can use the n() function to count the number of rows in a group. In this case, we're grouping the data by month (in the format month-year) using group_by(month), and then counting the number of rows in each group (i.e., the number of tweets in each month) using n()

tweet_sentiments_a <- afinn_scores %>% 
  group_by(month, year) %>% 
  summarise(sentiment = mean(value), n_tweets = n()) %>%
  #Changing the format of the month variable back into a date format so that we can use the geom_smooth function with it
  mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))


#Ensuring that the correct number of tweets are gathered for tweet_sentiments above 
#tweets_EN %>% filter(month == '10-14') 

#afinn_scores %>% filter(month == '10-14') 
```


### Code to look at average sentiment by month from 2014-2023 on a single graph (minus Chinese state media sources)
```{r}
tweet_sentiments_a %>% ggplot(aes(month, sentiment)) +
  
  #geom_smooth(color = "red", linetype = "solid", se = FALSE) +
  geom_line(color = "blue", size = 1) +
  geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue") +
  
  # Add vertical lines for each year (like x = 3)
  geom_vline(aes(xintercept = as.numeric(as.Date(paste0(year, "-01-01")))), 
             color = "gray", linetype = "dotted") +            
  labs(
    title = "Average Sentiment Score of Tweets by month from 2014-2023", 
    x = "Time", 
    y = "Sentiment Score (-6 to 6)",
    size = "Tweet Size")+
  
  #scale_y_continuous(
    #breaks = seq(-1, 1, by = .2),
    #limits = c(-1, 1)) +
  
  #Organizes the x axis by each month then formats how we want it labeled 
   scale_x_date(
    date_breaks = "1 year",
    date_labels = "%b '%y",
    expand = c(0.1, 1)) +
    #date_minor_breaks = "1 month", minor_breaks = "1 month") +
  
  theme_clean() +
  
  theme(panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
    legend.position = c(.7, .8),
    legend.direction = "horizontal",
    legend.title = element_text(size = 10.5), 
    legend.text = element_text(size = 10))

#ggsave(one, file = "Cool Figs/AvgSentAll.png")
```


### Code to create a graph for each year of sentiment analysis (specific year analysis)
```{r}
sentiment_year <- list()

for (yr in unique(tweet_sentiments_a$year)) {
  df_year <- subset(tweet_sentiments_a, year == yr) 
  
  plot <- ggplot(df_year, aes(month, sentiment)) +
  
  #geom_smooth(color = "red", linetype = "solid", se = FALSE) +
  geom_line(color = "blue", size = 1) +
  geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue") +
    
  # Add vertical lines for each month
  geom_vline(aes(xintercept = month), color = "gray", linetype = "dotted") +

  labs(
    title = paste0("Average Sentiment Score of Tweets by Month in ", yr), 
    x = paste0("Month (", yr, ")"), 
    y = "Sentiment Score (-6 to 6)",
    size = "Tweet Size"
    )+
  
  #scale_y_continuous(
    #breaks = seq(-1, 1, by = .2),
    #limits = c(-1, 1)) +
  
  #Organizes the x axis by each month then formats how we want it labeled 
   scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b") +
    #date_minor_breaks = "1 month", minor_breaks = "1 month") +
  
  theme_clean() +
  
  theme(
    panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
    
    legend.position = c(.7, .8),
    legend.direction = "horizontal",
    legend.title = element_text(size = 10.5), 
    legend.text = element_text(size = 10) 
    ) 
  
  sentiment_year[[yr]] <- plot
  
  rm(plot, df_year)
    
}

sentiment_year[["2018"]]


#plot_grid(plotlist = sentiment_year, ncol = 3)
```



### Sentiment Analysis of the top 5, 10, and 20 twitter accounts posting about China's BRI (group)
```{r}
#Investigating top 5, 10, and 20 twitter accounts who's tweets contain sentiments that match our lexicon list and therefore are more influential in the data 

#code that gathers the top 5, 10, and 20 users that tweeted about China's BRI and creates a temporary data frame 
top_20_afinn <- afinn_scores %>% count(username, sort = TRUE) %>% top_n(20)
top_10_afinn <- afinn_scores %>% count(username, sort = TRUE) %>% top_n(10)
top_5_afinn <- afinn_scores %>% count(username, sort = TRUE) %>% top_n(5)

#Code for separate sentiment analysis just on the top tweeters to see how they are influencing the conversation

#Extracting the distinct usernames from the top 5, 10, and 20 df's respectively and putting the usernames into a list
top_20_a <- top_20_afinn$username
top_10_a <- top_10_afinn$username
top_5_a <- top_5_afinn$username

#tidying up 
rm(top_10_afinn, top_5_afinn, top_20_afinn)
```


```{r}
#Looking at the sentiment of our top 10 tweeters (gathered from code above) to see how they are influencing the conversation

#Note that the the top tweeters does not necessarily translate to the top tweeters who displayed some sentiment about belt and Road 
#hence, the drastic decline of user "Shyam17"

O_list <- list(top_5_a, top_10_a, top_20_a)
names(O_list) <- c("top_5", "top_10", "top_20")

TopX_list <- list()

for (list in names(O_list)) {
  topx <- afinn_scores %>% 
  filter(username %in% O_list[[list]]) %>% 
  group_by(month) %>% 
  summarise(sentiment = mean(value), n_tweets = n()) %>%
  mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))
  
  TopX_list[[list]] <- topx
} 

TopX_list[["top_10"]]

rm(O_list, topx, list)  

#verifiying that only the top 10 usernames saved 
#sentiments_top10 %>% count(username, sort = TRUE)
```


#### Graphing top 5, 10, and 20 twitter accounts sentiment individually
```{r}
#Graphing top 5, 10, and 20 twitter accounts sentiment individually

topx_graphs <- list()

for (topx in names(TopX_list)) {

plot <-ggplot(TopX_list[[topx]], aes(month, sentiment)) +
  
  geom_line(color = "blue", size = 1) +
  geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue") +
  
  labs(
    title = paste0("Average Sentiment Score by ", topx ," Tweeters over time"), 
    x = "Month", 
    y = "Sentiment Score (-6 to 6)",
    size = "Tweet Size"
    )+
  
  #scale_y_continuous(
    #breaks = seq(-.8, 1.2, by = .2),
    #limits = c(-.8, 1.2)) +
  
  #Organizes the x axis by each month then formats how we want it labeled 
   scale_x_date(
    date_breaks = "1 year",
    date_labels = "%b '%y") +
    #date_minor_breaks = "1 month", minor_breaks = "1 month") +
  
  theme_clean() +
  
  theme(
    panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
    
    legend.position = c(.6, .8),
    legend.direction = "horizontal",
    legend.title = element_text(size = 10.5), 
    legend.text = element_text(size = 10) 
    ) 
  
  topx_graphs[[topx]] <- plot
}

topx_graphs[["top_10"]]

rm(plot, topx)
```


#### Looking at top 5, 10, and 20 twitter accounts sentiment on the same graph 
```{r}
# I don't use use top_n here because then it would compared relative size of tweets to it's own data frame (making it look like top_20 had less tweets)

ggplot() +
  
  geom_point(data = TopX_list[["top_5"]], aes(month, sentiment, color = "Top 5"), size = 2) +
  geom_smooth(data = TopX_list[["top_5"]], aes(month, sentiment), se = FALSE, color = "green") +
  
  geom_point(data = TopX_list[["top_10"]], aes(month, sentiment, color = "Top 10"), size = 2) +
  geom_smooth(data = TopX_list[["top_10"]], aes(month, sentiment), se = FALSE, color = "red") +
 
  geom_point(data = TopX_list[["top_20"]], aes(month, sentiment, color = "Top 20"), size = 2) +
  geom_smooth(data = TopX_list[["top_20"]], aes(month, sentiment), se = FALSE, color = "blue") +
  
  scale_color_manual(values = c("Top 5" = "green", "Top 10" = "red",  "Top 20" = "blue")) + 
                     #labels = c("Top 5", "Top 10", "top 20")) +
  
  labs(
    title = "Average Sentiment Score by Tweeters", 
    x = "Year", 
    y = "Sentiment Score (-6 to 6)",
    color = NULL
    )+
  
  scale_y_continuous(
    breaks = seq(-2, 2, by = 1),
    limits = c(-2, 2)) +
  
  #Organizes the x axis by each month then formats how we want it labeled 
   scale_x_date(
    date_breaks = "1 year",
    date_labels = "%b '%y") +
    #date_minor_breaks = "1 month", minor_breaks = "1 month") +
  
  theme_clean() +
  
  theme(panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
    legend.position = c(.5, .2),
    legend.direction = "horizontal",
    legend.text = element_text(size = 10)) 
```


### Sentiment Analysis of the specific top 20 twitter accounts posting about China's BRI (looking at the individual users)
```{r}
top_users_list <- list()

#Creating 20 different dataframes based on the top 20 tweeters  

for (user in top_20_a) {
  hum <- afinn_scores %>% filter(username == user) %>%
    group_by(month) %>% 
    summarise(sentiment = mean(value), n_tweets = n()) %>%
    mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))
  
  top_users_list[[user]] <- hum 
  
  rm(hum)
}

#list of top 20 users to choose from 
names(top_users_list)

top_users_list[["orfonline"]]
```


```{r}
#Code to iterate through the top 20 tweeters to create the graphs for each user

top20_user_plots <- list()

# Loop through time data frames and create plots for the factor varaible teeth 
for (user in names(top_users_list)) { #Equivalent of "range(len(dfs_times))" in python 
  
  plot <- ggplot(top_users_list[[user]], aes(month, sentiment)) +
    geom_line(color = "red", size = 1) +
    geom_point(aes(size = n_tweets), alpha = 0.6, color = "red") +
    
    labs(
    title = paste0("Username: " , user) , 
    x = "Month", 
    y = "Sentiment Score (-6 to 6)") +
    
    #Organizes the x axis by each month then formats how we want it labeled 
    scale_x_date(
      date_breaks = "1 year",
      date_labels = "%b '%y") +
    
    theme_clean()
    
  
    top20_user_plots[[user]] <- plot
}

names(top20_user_plots)

top20_user_plots[["raghavan1314"]]

rm(plot)

#plot_grid(plotlist = top20_user_plots, ncol = 6)
```


### Analysis of only "china state-affiliated media" or "China government organization" (designation based on twitter)
```{r}
#taking all Chinese state media twitter accounts and putting them in a list 
#Already done above, so to avoid redundancy here's the list again! 

china_offical_news 
```


```{r}
#Looking at the sentiment of only Chinese state media twitter accounts to see how they are influencing the conversation

#Note that the the top tweeters does not necessarily translate to the top tweeters who displayed some sentiment about belt and Road 

sentiments_CSM <- afinn_scores_b %>% 
  filter(username %in% china_offical_news) %>% 
  group_by(month) %>% 
  summarise(sentiment = mean(value), n_tweets = n()) %>%
  mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))


#verifiying that only the top 10 usernames saved 
#sentiments_top10 %>% count(username, sort = TRUE)
```


#### Graphing all Chinese news outlets together 
```{r}
sentiments_CSM %>% ggplot(aes(month, sentiment)) +
  
  geom_line(color = "blue", size = 1) +
  geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue") +
  
  labs(
    title = "Average Sentiment Score by All Offical Chinese State Media", 
    x = "Year", 
    y = "Sentiment Score (-6 to 6)",
    size = "Tweet Size"
    )+
  
  scale_y_continuous(
    breaks = seq(-2, 2, by = 1),
    limits = c(-2, 2)) +
  
  #Organizes the x axis by each month then formats how we want it labeled 
   scale_x_date(
    date_breaks = "1 year",
    date_labels = "%b '%y") +

  theme_clean() +
  
  theme(panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
    legend.position = c(.5, .3),
    legend.direction = "horizontal",
    legend.title = element_text(size = 10.5), 
    legend.text = element_text(size = 10))

#ggsave(one, file = "Cool Figs/CSM_All.png")
```


#### Graphing the top 4 Chinese news outlets individually on the same graph to see which outlets are the most positive in terms of BRI
```{r}
outlets <- list()

for (outlet in china_offical_news) {
  CSM_outlet <- afinn_scores_b %>% 
  filter(username == outlet) %>% 
  group_by(month) %>% 
  summarise(sentiment = mean(value), n_tweets = n()) %>%
  mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))

outlets[[outlet]] <- CSM_outlet
}

china_offical_news

outlets[["CCTV"]]
```


```{r}
#graph is scrunched on y axis to focus on smoothing line

ggplot() +
  
  geom_point(data = outlets[["beltroadnews"]], aes(month, sentiment, color = "beltroadnews"), size = 3) +
  geom_smooth(data = outlets[["beltroadnews"]], aes(month, sentiment), se = FALSE, color = "red")+

  geom_point(data = outlets[["XHNews"]], aes(month, sentiment, color = "XHNews"), size = 3) +
  geom_smooth(data = outlets[["XHNews"]], aes(month, sentiment), se = FALSE, color = "blue") +
  
  geom_point(data = outlets[["globaltimesnews"]], aes(month, sentiment, color = "globaltimesnews"), size = 3) +
  geom_smooth(data = outlets[["globaltimesnews"]], aes(month, sentiment), se = FALSE, color = "green") +
  
  geom_point(data = outlets[["ChinaDaily"]], aes(month, sentiment, color = "ChinaDaily"), size = 3) +
  geom_smooth(data = outlets[["ChinaDaily"]], aes(month, sentiment), se = FALSE, color = "orange") +
  
  scale_color_manual(values = c("beltroadnews" = "red", "XHNews" = "blue",  "globaltimesnews" = "green", "ChinaDaily" = "orange")) +
  
  labs(
    title = "Average Sentiment Score of Select CSM", 
    x = "Year", 
    y = "Sentiment Score (-6 to 6)",
    color = "News Agency") +
  
  scale_y_continuous(
    breaks = seq(0, 2, by = .2),
    limits = c(0, 2)) +
  
  #Organizes the x axis by each month then formats how we want it labeled 
   scale_x_date(
    date_breaks = "1 year",
    date_labels = "%b '%y") +
  
  theme_clean()

#ggsave(one, file = "Cool Figs/SelectCSM.png", width = 12, height =7)
```



### combined plot - all tweeters vs top 10 tweeters vs top 5 tweeters vs Chinese state media 
```{r}
#Graph plotting Top 5, top 10, all tweets, and Chinese state media tweets together on the same plot to see their influence 

ggplot() +
  
  geom_point(data = tweet_sentiments_a, aes(month, sentiment, color = "All Tweets"), size = 3) +
  geom_smooth(data = tweet_sentiments_a, aes(month, sentiment), se = FALSE, color = "red")+

  geom_point(data = TopX_list[["top_5"]], aes(month, sentiment, color = "Top 5"), size = 3) +
  geom_smooth(data = TopX_list[["top_5"]], aes(month, sentiment), se = FALSE, color = "blue") +
  
#  geom_point(data = TopX_list[["top_10"]], aes(month, sentiment, color = "Top 10"), size = 3) +
#  geom_smooth(data = TopX_list[["top_10"]], aes(month, sentiment), se = FALSE, color = "purple") +
 
#  geom_point(data = TopX_list[["top_20"]], aes(month, sentiment, color = "Top 20"), size = 3) +
#  geom_smooth(data = TopX_list[["top_20"]], aes(month, sentiment), se = FALSE, color = "green") +
  
  geom_point(data = sentiments_CSM, aes(month, sentiment, color = "Chinese State Media"), size = 3) +
  geom_smooth(data = sentiments_CSM, aes(month, sentiment), se = FALSE, color = "orange") +
  
  scale_color_manual(values = c("All Tweets" = "red", "Top 5" = "blue", "Top 10" = "purple", "Top 20" = "green", "Chinese State Media" = "orange")) + 
                     #labels = c("All Tweeters", "Top Ten", "Top Five", "CSM")) +
  labs(
    title = "Average Sentiment Score from 2014-2022", 
    x = "Year", 
    y = "Sentiment Score (-6 to 6)",
    color = NULL) +
  
  #scale_y_continuous(
    #breaks = seq(-0.5, 2, by = .2),
    #limits = c(-0.5, 2)) +
  
  #Organizes the x axis by each month then formats how we want it labeled 
   scale_x_date(
    date_breaks = "1 year",
    date_labels = "%b '%y") +
  
  theme_clean() +
  theme(panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
    legend.position = c(.7, .9),
    legend.direction = "horizontal",
    legend.title = element_text(size = 10.5), 
    legend.text = element_text(size = 10))

#ggsave(one, file = "Cool Figs/Top5CSMAll.png")
```


### Same graph as above just focusing on the difference between All tweeterss and chinese State media (with confidence intervals) 
```{r}
ggplot() +
  
  geom_point(data = tweet_sentiments_a, aes(month, sentiment, color = "All Tweets"), size = 3) +
  geom_smooth(data = tweet_sentiments_a, aes(month, sentiment), color = "red")+

#  geom_point(data = TopX_list[["top_5"]], aes(month, sentiment, color = "Top 5"), size = 3) +
#  geom_smooth(data = TopX_list[["top_5"]], aes(month, sentiment), se = FALSE, color = "blue") +
  
#  geom_point(data = TopX_list[["top_10"]], aes(month, sentiment, color = "Top 10"), size = 3) +
#  geom_smooth(data = TopX_list[["top_10"]], aes(month, sentiment), se = FALSE, color = "purple") +
 
#  geom_point(data = TopX_list[["top_20"]], aes(month, sentiment, color = "Top 20"), size = 3) +
#  geom_smooth(data = TopX_list[["top_20"]], aes(month, sentiment), se = FALSE, color = "green") +
  
  geom_point(data = sentiments_CSM, aes(month, sentiment, color = "Chinese State Media"), size = 3) +
  geom_smooth(data = sentiments_CSM, aes(month, sentiment), color = "orange") +
  
  scale_color_manual(values = c("All Tweets" = "red", "Top 5" = "blue", "Top 10" = "purple", "Top 20" = "green", "Chinese State Media" = "orange")) + 
                     #labels = c("All Tweeters", "Top Ten", "Top Five", "CSM")) +
  labs(
    title = "Average Sentiment Score from 2014-2023", 
    x = "Month", 
    y = "Sentiment Score (-6 to 6)",
    color = "Tweet Source") +
  
  scale_y_continuous(
    breaks = seq(-2, 2, by = 1),
    limits = c(-2, 2)) +
  
  #Organizes the x axis by each month then formats how we want it labeled 
   scale_x_date(
    date_breaks = "1 year",
    date_labels = "%b '%y",
    expand = c(0.1, 1)) +
  
  theme_clean() +
  theme(panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
    legend.position = c(.7, .23),
    legend.direction = "horizontal",
    legend.title = element_text(size = 10.5), 
    legend.text = element_text(size = 10))

#ggsave(one, file = "Cool Figs/AllTweetsVCSM.png", width = 9, height = 6) #trying out different dimensions 
```




## Bing visualizations


### Preprocessing bing_scores for visualization of sentiment over all years in dataset
```{r}
#standardizing the bing lexicon to compare (positive = 1, negative = -1) then averaging them out to plot them over time 

bing_scores$stan <- ifelse(bing_scores$sentiment == "positive", 1, -1)
```


#### Also modifying the affin dataset to be on the same standardized scalle as the bing dataset for comparison
```{r}
#Creating a dataframe called tweet_sentiments that contains the month, average sentiment of tweets for that month, and number of tweets

#n_tweets = n() is used to create a new column in the summarized data frame that contains the number of tweets per month.
#When using dplyr functions like summarize(), you can use the n() function to count the number of rows in a group. In this case, we're grouping the data by month (in the format month-year) using group_by(month), and then counting the number of rows in each group (i.e., the number of tweets in each month) using n()

tweet_sentiments_b <- bing_scores %>% 
  group_by(month, year) %>% 
  summarise(sentiment = mean(stan), n_tweets = n()) %>%
  #Changing the format of the month variable back into a date format so that we can use the geom_smooth function with it
  mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))


#modification of afinn tweet_sentiments to be based on standardized column for comparison against bing
tweet_sentiments_a_s <- afinn_scores %>% 
  group_by(month, year) %>% 
  summarise(sentiment = mean(stan), n_tweets = n()) %>%
  #Changing the format of the month variable back into a date format so that we can use the geom_smooth function with it
  mutate(month = as.Date(paste0("01-", month), format = "%d-%m-%y"))
```


### Looking at average sentiment from 2014-2023 on a single graph (minus Chinese state media sources)
```{r}
tweet_sentiments_b %>% ggplot(aes(month, sentiment)) +
  
  #geom_smooth(color = "red", linetype = "solid", se = FALSE) +
  geom_line(color = "blue", size = 1) +
  geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue") +
  
  # Add vertical lines for each year
  geom_vline(aes(xintercept = as.numeric(as.Date(paste0(year, "-01-01")))), 
             color = "gray", linetype = "dotted") +            
  labs(
    title = "Average Sentiment of Tweets Using Bing Lexicon", 
    x = "Year", 
    y = "Sentiment Score (-1 to 1)",
    size = "Tweet Size"
    )+
  
  #scale_y_continuous(
    #breaks = seq(-1, 1, by = .2),
    #limits = c(-1, 1)) +
  
  #Organizes the x axis by each month then formats how we want it labeled 
   scale_x_date(
    date_breaks = "1 year",
    date_labels = "%b '%y") +
    #date_minor_breaks = "1 month", minor_breaks = "1 month") +
  
  theme_clean() +
  
  theme(panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
    legend.position = c(.7, .2),
    legend.direction = "horizontal",
    legend.title = element_text(size = 10.5), 
    legend.text = element_text(size = 10)) 

#ggsave(one, file = "Cool Figs/AvgSentAllBing.png")
```

### Code to create a graph for each year of sentiment analysis (specific year analysis)
```{r}
sentiment_year_b <- list()

for (yr in unique(tweet_sentiments_b$year)) {
  df_year <- subset(tweet_sentiments_b, year == yr) 
  
  plot <- ggplot(df_year, aes(month, sentiment)) +
  
  #geom_smooth(color = "red", linetype = "solid", se = FALSE) +
  geom_line(color = "blue", size = 1) +
  geom_point(aes(size = n_tweets), alpha = 0.6, color = "blue") +
    
  # Add vertical lines for each month
  geom_vline(aes(xintercept = month), color = "gray", linetype = "dotted") +

  labs(
    title = paste0("Average Sentiment of Tweets (Bing) in ", yr), 
    x = paste0("Month (", yr, ")"), 
    y = "Sentiment Score (-6 to 6)",
    size = "Tweet Size"
    )+
  
  #scale_y_continuous(
    #breaks = seq(-1, 1, by = .2),
    #limits = c(-1, 1)) +
  
  #Organizes the x axis by each month then formats how we want it labeled 
   scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b") +
    #date_minor_breaks = "1 month", minor_breaks = "1 month") +
  
  theme_clean() +
  
  theme(panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
    #legend.position = c(.7, .3),
    #legend.direction = "horizontal",
    legend.title = element_text(size = 10.5), 
    legend.text = element_text(size = 10)) 
  
  sentiment_year_b[[yr]] <- plot
  
  rm(plot, df_year)
    
}

sentiment_year_b[["2018"]]


#plot_grid(plotlist = sentiment_year, ncol = 3)
```


#graph that compares overall analysis of bing and afinn lexicons on our dataset
```{r}

ggplot() +
  
  geom_line(data = tweet_sentiments_a_s, aes(month, sentiment, color = "All Tweets (Afinn)"), size = 1) +
  
  geom_line(data = tweet_sentiments_b, aes(month, sentiment, color = "All Tweets (Bing)"), size = 1) +
  
  scale_color_manual(values = c("All Tweets (Afinn)" = "red", "All Tweets (Bing)" = "blue")) + 
  
  labs(
    title = "Average Sentiment Score of Tweets from 2014 - 2023",
    subtitle = "Sentiment standardized on -1 to 1 scale", 
    x = "Year", 
    y = "Sentiment Score (-1 to 1)",
    size = "Tweet Size",
    color = "Lexicon")+
  
  #scale_y_continuous(
    #breaks = seq(-2, 2, by = .2),
    #limits = c(-2, 2)) +
  
  #Organizes the x axis by each month then formats how we want it labeled 
   scale_x_date(
    date_breaks = "1 year",
    date_labels = "%b '%y") +
    #date_minor_breaks = "1 month", minor_breaks = "1 month") +
  
  theme_clean() +
  
  theme(panel.grid.minor.y = element_line(color = "gray", linetype = "dotted"),
    legend.position = c(.7, .9),
    legend.direction = "horizontal",
    legend.title = element_text(size = 10.5), 
    legend.text = element_text(size = 10)) 

#ggsave(one, file = "Cool Figs/AfinnVBingAll.png", width = 16, height = 9) #trying out different dimensions 16 by 9 
```



```{r eval=FALSE, include=FALSE}
# Present results

#Saving excel chart
library(writexl)
write_xlsx(afinn_scores, "afinn_scores.xlsx")


```